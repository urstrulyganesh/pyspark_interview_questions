{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce325eb-5abf-4942-85c7-535c737c767c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7431f43c-95a8-4cf0-817a-a46101053c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "You are given a dataset containing sales data for different stores across various months. Each row contains the store name, the month, and the sales amount. Your task is to calculate the cumulative sales for each store, considering the monthly sales, using PySpark.\n",
    "\n",
    "You should also:\n",
    "Filter out stores with sales lower than 1000 in any month.\n",
    "Calculate the total sales for each store over all months.\n",
    "Sort the results by the total sales in descending order.\n",
    "\n",
    "data = [ (\"Store A\", \"2024-01\", 800), (\"Store A\", \"2024-02\", 1200), (\"Store A\", \"2024-03\", 900), (\"Store B\", \"2024-01\", 1500), (\"Store B\", \"2024-02\", 1600), (\"Store B\", \"2024-03\", 1400), (\"Store C\", \"2024-01\", 700), (\"Store C\", \"2024-02\", 1000), (\"Store C\", \"2024-03\", 800) ] \n",
    "\n",
    "Create DataFrame df = spark.createDataFrame(data, [\"Store\", \"Month\", \"Sales\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862fc97f-f2cd-490e-8ded-e337a294c094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2\n",
    "\n",
    "You are given a dataset containing sales data for different stores across various months. Each row contains the store name, the month, and the sales amount. Your task is to calculate the cumulative sales for each store, considering the monthly sales, using PySpark.\n",
    "\n",
    "You should also:\n",
    "Filter out stores with sales lower than 1000 in any month.\n",
    "Calculate the total sales for each store over all months.\n",
    "Sort the results by the total sales in descending order.\n",
    "\n",
    "\uD835\uDC2C\uD835\uDC1C\uD835\uDC21\uD835\uDC1E\uD835\uDC26\uD835\uDC1A \uD835\uDC1A\uD835\uDC27\uD835\uDC1D \uD835\uDC1D\uD835\uDC1A\uD835\uDC2D\uD835\uDC1A\uD835\uDC2C\uD835\uDC1E\uD835\uDC2D\n",
    "data = [ (\"Store A\", \"2024-01\", 800), (\"Store A\", \"2024-02\", 1200), (\"Store A\", \"2024-03\", 900), (\"Store B\", \"2024-01\", 1500), (\"Store B\", \"2024-02\", 1600), (\"Store B\", \"2024-03\", 1400), (\"Store C\", \"2024-01\", 700), (\"Store C\", \"2024-02\", 1000), (\"Store C\", \"2024-03\", 800) ] \n",
    "\n",
    "Create DataFrame df = spark.createDataFrame(data, [\"Store\", \"Month\", \"Sales\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf864f8-8437-4919-96e3-f35a7737e47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "102d276a-af8d-4369-b12e-5c47bb2d0063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are given two datasets — one containing employee details such as name, department, salary, and joining date, and another containing their job titles. Your task is to identify the employees who have the highest and lowest salary across the company. The final output should display their name, job title, salary, and a new column called salary_type that indicates whether the salary is the \"Highest Salary\" or the \"Lowest Salary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d86a4f-082b-46c2-9f6d-1dee94281ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "worker:\n",
    "|worker_id|first_name|last_name|salary|joining_date| department|\n",
    "| 1| John| Doe| 5000| 2023–01–01|Engineering|\n",
    "| 2| Jane| Smith| 6000| 2023–01–15| Marketing|\n",
    "| 3| Alice| Johnson| 4500| 2023–02–05|Engineering|\n",
    "\n",
    "\n",
    "title:\n",
    "|worker_ref_id|worker_title|affected_from|\n",
    "| 1| Engineer| 2022–01–01|\n",
    "| 2| Manager| 2022–01–01|\n",
    "| 3| Engineer| 2022–01–01|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7adfc024-c894-49d6-9c6e-021fc89910d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b74633c-2c84-4281-bdb8-6bdb30327a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create DataFrames\n",
    "worker_data = [\n",
    "    (1, \"John\", \"Doe\", 5000, \"2023-01-01\", \"Engineering\"),\n",
    "    (2, \"Jane\", \"Smith\", 6000, \"2023-01-15\", \"Marketing\"),\n",
    "    (3, \"Alice\", \"Johnson\", 4500, \"2023-02-05\", \"Engineering\")\n",
    "]\n",
    "title_data = [\n",
    "    (1, \"Engineer\", \"2022-01-01\"),\n",
    "    (2, \"Manager\", \"2022-01-01\"),\n",
    "    (3, \"Engineer\", \"2022-01-01\")\n",
    "]\n",
    "\n",
    "df_worker = spark.createDataFrame(worker_data, [\"worker_id\", \"first_name\", \"last_name\", \"salary\", \"joining_date\", \"department\"])\n",
    "df_title = spark.createDataFrame(title_data, [\"worker_ref_id\", \"worker_title\", \"affected_from\"])\n",
    "\n",
    "# Use window functions to rank by salary\n",
    "from pyspark.sql.functions import dense_rank\n",
    "window_spec = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df_ranked = df_worker.withColumn(\"rank_high\", dense_rank().over(window_spec)) \\\n",
    "                     .withColumn(\"rank_low\", dense_rank().over(Window.orderBy(col(\"salary\").asc())))\n",
    "\n",
    "# Filter highest and lowest salaries\n",
    "df_filtered = df_ranked.filter((col(\"rank_high\") == 1) | (col(\"rank_low\") == 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9124e60d-2c8b-47dd-bb9e-a13be2faac37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>worker_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>joining_date</th><th>department</th><th>rank_high</th><th>rank_low</th></tr></thead><tbody><tr><td>3</td><td>Alice</td><td>Johnson</td><td>4500</td><td>2023-02-05</td><td>Engineering</td><td>3</td><td>1</td></tr><tr><td>2</td><td>Jane</td><td>Smith</td><td>6000</td><td>2023-01-15</td><td>Marketing</td><td>1</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Alice",
         "Johnson",
         4500,
         "2023-02-05",
         "Engineering",
         3,
         1
        ],
        [
         2,
         "Jane",
         "Smith",
         6000,
         "2023-01-15",
         "Marketing",
         1,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "worker_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "joining_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank_high",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rank_low",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_filtered.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d92d1d-a4b9-4193-896c-2784cdb48368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered=df_filtered.withColumn(\"salary_type\",when(col(\"rank_high\")==1,\"Highest Salary\").when(col(\"rank_low\")==1,\"Lowest_Salary\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9463851c-615a-476f-a42e-387dc4b4ec11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>worker_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>joining_date</th><th>department</th><th>rank_high</th><th>rank_low</th><th>salary_type</th></tr></thead><tbody><tr><td>3</td><td>Alice</td><td>Johnson</td><td>4500</td><td>2023-02-05</td><td>Engineering</td><td>3</td><td>1</td><td>Lowest_Salary</td></tr><tr><td>2</td><td>Jane</td><td>Smith</td><td>6000</td><td>2023-01-15</td><td>Marketing</td><td>1</td><td>3</td><td>Highest Salary</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Alice",
         "Johnson",
         4500,
         "2023-02-05",
         "Engineering",
         3,
         1,
         "Lowest_Salary"
        ],
        [
         2,
         "Jane",
         "Smith",
         6000,
         "2023-01-15",
         "Marketing",
         1,
         3,
         "Highest Salary"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "worker_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "joining_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank_high",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rank_low",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary_type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_filtered.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f61179-08a2-4153-b5d1-a4592a0dc1a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>worker_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>worker_title</th><th>salary_type</th></tr></thead><tbody><tr><td>3</td><td>Alice</td><td>Johnson</td><td>4500</td><td>Engineer</td><td>Lowest_Salary</td></tr><tr><td>2</td><td>Jane</td><td>Smith</td><td>6000</td><td>Manager</td><td>Highest Salary</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Alice",
         "Johnson",
         4500,
         "Engineer",
         "Lowest_Salary"
        ],
        [
         2,
         "Jane",
         "Smith",
         6000,
         "Manager",
         "Highest Salary"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "worker_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "worker_title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary_type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final=df_filtered.join(df_title,df_filtered.worker_id==df_title.worker_ref_id,\"inner\")\\\n",
    "    .select(\"worker_id\", \"first_name\", \"last_name\", \"salary\", \"worker_title\", \"salary_type\")\\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d908fc71-b2c4-42ab-a750-c664c38a0ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4\n",
    "Pivot the Column into Rows\n",
    "\uD835\uDC2C\uD835\uDC1C\uD835\uDC21\uD835\uDC1E\uD835\uDC26\uD835\uDC1A \uD835\uDC1A\uD835\uDC27\uD835\uDC1D \uD835\uDC1D\uD835\uDC1A\uD835\uDC2D\uD835\uDC1A\uD835\uDC2C\uD835\uDC1E\uD835\uDC2D\n",
    "Given Input -\n",
    "\n",
    "StudentID, StudentName , AScore, BScore,CScore\n",
    "123, A, 30, 31, 32\n",
    "124, B, 40, 41, 42\n",
    "Get the output in below format -\n",
    "\n",
    "StudentID, StudentName , Subject , Score\n",
    "123, A, AScore, 30\n",
    "123, A, BScore, 31\n",
    "123, A, CScore, 32\n",
    "124, B, AScore, 40\n",
    "124, B, BScore, 41\n",
    "124, B, CScore, 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b5d125-5b40-422a-8345-842c65b5cebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"PivotColumnsToRows\").getOrCreate()\n",
    "\n",
    "# Input Data\n",
    "data = [\n",
    "    (123, \"A\", 30, 31, 32),\n",
    "    (124, \"B\", 40, 41, 42)\n",
    "]\n",
    "columns = [\"StudentID\", \"StudentName\", \"AScore\", \"BScore\", \"CScore\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90cc1ce-3b74-4731-9dbd-9647b986291e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>StudentID</th><th>StudentName</th><th>AScore</th><th>BScore</th><th>CScore</th></tr></thead><tbody><tr><td>123</td><td>A</td><td>30</td><td>31</td><td>32</td></tr><tr><td>124</td><td>B</td><td>40</td><td>41</td><td>42</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         123,
         "A",
         30,
         31,
         32
        ],
        [
         124,
         "B",
         40,
         41,
         42
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "StudentID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "StudentName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "AScore",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "BScore",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CScore",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb87c8c-a63c-4847-b0f2-ba403108055e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_unpivoted = df.selectExpr(\n",
    "    \"StudentID\", \"StudentName\",\n",
    "    \"stack(3, 'AScore', AScore, 'BScore', BScore, 'CScore', CScore) as (Subject, Score)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e634439f-b553-4ad9-b38d-9f6f8698dc01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>StudentID</th><th>StudentName</th><th>Subject</th><th>Score</th></tr></thead><tbody><tr><td>123</td><td>A</td><td>AScore</td><td>30</td></tr><tr><td>123</td><td>A</td><td>BScore</td><td>31</td></tr><tr><td>123</td><td>A</td><td>CScore</td><td>32</td></tr><tr><td>124</td><td>B</td><td>AScore</td><td>40</td></tr><tr><td>124</td><td>B</td><td>BScore</td><td>41</td></tr><tr><td>124</td><td>B</td><td>CScore</td><td>42</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         123,
         "A",
         "AScore",
         30
        ],
        [
         123,
         "A",
         "BScore",
         31
        ],
        [
         123,
         "A",
         "CScore",
         32
        ],
        [
         124,
         "B",
         "AScore",
         40
        ],
        [
         124,
         "B",
         "BScore",
         41
        ],
        [
         124,
         "B",
         "CScore",
         42
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "StudentID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "StudentName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Subject",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Score",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_unpivoted.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca65ef6-f421-4ace-b091-8ece73c0ff28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5\n",
    "Handle Missing Data Efficiently\n",
    "You have a DataFrame containing sales data. Write a PySpark script to:\n",
    "Replace missing values in the price column with the mean price.\n",
    "Drop rows where the product column is null.\n",
    "Fill missing values in the quantity column with 1.\n",
    "\n",
    "data = [ (1, \"Laptop\", 1000, 5), (2, \"Mouse\", None, None), (3, \"Keyboard\", 50, 2), (4, \"Monitor\", 200, None), (5, None, 500, None), ] \n",
    "\n",
    "Define schema and create DataFrame \n",
    "columns = [\"product_id\", \"product\", \"price\", \"quantity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b6f8c3-40c3-417e-82a0-cd7f2c93880e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Laptop\", 1000, 5),\n",
    "    (2, \"Mouse\", None, None),\n",
    "    (3, \"Keyboard\", 50, 2),\n",
    "    (4, \"Monitor\", 200, None),\n",
    "    (5, None, 500, None),\n",
    "]\n",
    "columns = [\"product_id\", \"product\", \"price\", \"quantity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032b5a45-672a-4bf2-a4f1-fc7de2eacd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d013f0-9a52-48e6-96d5-a0408c49d95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are given a dataset containing sales information with some missing values. Your task is to handle these nulls efficiently using PySpark:\n",
    "\n",
    "Replace missing values in the price column with the average price.\n",
    "\n",
    "Drop any row where the product column is null.\n",
    "\n",
    "Fill missing values in the quantity column with 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b4e03b-1a9b-45ff-ab0a-ee70b3634400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>1000</td><td>5</td></tr><tr><td>2</td><td>Mouse</td><td>null</td><td>null</td></tr><tr><td>3</td><td>Keyboard</td><td>50</td><td>2</td></tr><tr><td>4</td><td>Monitor</td><td>200</td><td>null</td></tr><tr><td>5</td><td>null</td><td>500</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Laptop",
         1000,
         5
        ],
        [
         2,
         "Mouse",
         null,
         null
        ],
        [
         3,
         "Keyboard",
         50,
         2
        ],
        [
         4,
         "Monitor",
         200,
         null
        ],
        [
         5,
         null,
         500,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63e29fb2-2ed0-4771-8c38-030276b7155a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>1000</td><td>5</td></tr><tr><td>2</td><td>Mouse</td><td>null</td><td>null</td></tr><tr><td>3</td><td>Keyboard</td><td>50</td><td>2</td></tr><tr><td>4</td><td>Monitor</td><td>200</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Laptop",
         1000,
         5
        ],
        [
         2,
         "Mouse",
         null,
         null
        ],
        [
         3,
         "Keyboard",
         50,
         2
        ],
        [
         4,
         "Monitor",
         200,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=df.dropna(subset=[\"product\"])\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f383f021-886f-4af4-89a6-92d1bf7f9c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_avg=df.select(avg(\"price\")).first()[0]\n",
    "df=df.fillna({\"price\":df_avg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58dc136-dcc7-45c2-b8b6-630d3750bc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>1000</td><td>5</td></tr><tr><td>2</td><td>Mouse</td><td>416</td><td>null</td></tr><tr><td>3</td><td>Keyboard</td><td>50</td><td>2</td></tr><tr><td>4</td><td>Monitor</td><td>200</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Laptop",
         1000,
         5
        ],
        [
         2,
         "Mouse",
         416,
         null
        ],
        [
         3,
         "Keyboard",
         50,
         2
        ],
        [
         4,
         "Monitor",
         200,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d53585-db2b-48a5-9fd4-be2216f2f103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product</th><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>1000</td><td>5</td></tr><tr><td>2</td><td>Mouse</td><td>416</td><td>1</td></tr><tr><td>3</td><td>Keyboard</td><td>50</td><td>2</td></tr><tr><td>4</td><td>Monitor</td><td>200</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Laptop",
         1000,
         5
        ],
        [
         2,
         "Mouse",
         416,
         1
        ],
        [
         3,
         "Keyboard",
         50,
         2
        ],
        [
         4,
         "Monitor",
         200,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=df.fillna({\"quantity\":1})\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a332c78-72cc-4297-b219-63abab7cf9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1bc2172-4ba5-4a3b-bfa4-1b1c53a8b06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are provided with a DataFrame containing customer purchase data. Your task is to:\n",
    "Identify and remove duplicate rows based on the customer_id column.\n",
    "If duplicates exist for a customer_id, retain the row with the most recent purchase_date.\n",
    "\n",
    "\n",
    "\n",
    "data = [ (1, \"Laptop\", \"2024-08-01\"), (1, \"Mouse\", \"2024-08-05\"), (2, \"Keyboard\", \"2024-08-02\"), (2, \"Monitor\", \"2024-08-03\") ] \n",
    "\n",
    "columns = [\"customer_id\", \"product\", \"purchase_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1dd1ab-deb3-4f47-8e57-ccfa070baf5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Laptop\", \"2024-08-01\"),\n",
    "    (1, \"Mouse\", \"2024-08-05\"),\n",
    "    (2, \"Keyboard\", \"2024-08-02\"),\n",
    "    (2, \"Monitor\", \"2024-08-03\")\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"product\", \"purchase_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_6 = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97391699-bf6e-43b3-a03b-dd51bac979a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product</th><th>purchase_date</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>2024-08-01</td></tr><tr><td>1</td><td>Mouse</td><td>2024-08-05</td></tr><tr><td>2</td><td>Keyboard</td><td>2024-08-02</td></tr><tr><td>2</td><td>Monitor</td><td>2024-08-03</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Laptop",
         "2024-08-01"
        ],
        [
         1,
         "Mouse",
         "2024-08-05"
        ],
        [
         2,
         "Keyboard",
         "2024-08-02"
        ],
        [
         2,
         "Monitor",
         "2024-08-03"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "purchase_date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_6.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ed1cec-9e0d-4e0c-ab59-853dd6c06346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>product</th><th>purchase_date</th><th>rank</th></tr></thead><tbody><tr><td>1</td><td>Mouse</td><td>2024-08-05</td><td>1</td></tr><tr><td>2</td><td>Monitor</td><td>2024-08-03</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Mouse",
         "2024-08-05",
         1
        ],
        [
         2,
         "Monitor",
         "2024-08-03",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "purchase_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_6=df_6.withColumn(\"purchase_date\",col(\"purchase_date\").cast(\"date\"))\\\n",
    "    .withColumn(\"rank\",row_number().over(Window.partitionBy(\"customer_id\").orderBy(col(\"purchase_date\").desc())))\\\n",
    "    .filter(col(\"rank\")==1)\\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9726aee0-f61f-43ee-9bba-7ff0b809eaae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7.\n",
    "You are given product and price data:\n",
    "\n",
    "Add a new column price_category using the rules:\n",
    "\n",
    "\"Low\" if price < 100\n",
    "\n",
    "\"Medium\" if 100 ≤ price < 500\n",
    "\n",
    "\"High\" if price ≥ _500_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b206e24-26a0-4689-b479-1d9b1476ee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "data = [(\"Laptop\", 800), (\"Mouse\", 25), (\"Keyboard\", 150), (\"Monitor\", 300)] \n",
    "\n",
    "columns = [\"product\", \"price\"] [](url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d113aee-ab5c-47dd-a37e-d5a200e70611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1853\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[0;32m-> 1853\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n",
       "\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    270\u001B[0m     request: Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n",
       "\u001B[1;32m    278\u001B[0m         request,\n",
       "\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n",
       "\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n",
       "\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n",
       "\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m    284\u001B[0m     )\n",
       "\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n",
       "\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n",
       "\u001B[1;32m    331\u001B[0m )\n",
       "\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n",
       "\u001B[1;32m    316\u001B[0m         request,\n",
       "\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n",
       "\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n",
       "\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n",
       "\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n",
       "\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n",
       "\u001B[1;32m    322\u001B[0m     )\n",
       "\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m   1192\u001B[0m (\n",
       "\u001B[1;32m   1193\u001B[0m     state,\n",
       "\u001B[1;32m   1194\u001B[0m     call,\n",
       "\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n",
       "\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n",
       "\u001B[1;32m   1197\u001B[0m )\n",
       "\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n",
       "\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
       "\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-07-18T11:29:48.562756459+00:00\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7903817674957702>, line 12\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n",
       "\u001B[0;32m---> 12\u001B[0m df_7 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, columns)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:657\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m    655\u001B[0m         _schema \u001B[38;5;241m=\u001B[39m StructType()\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, _schema)\n",
       "\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 657\u001B[0m     _schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(_data, _cols)\n",
       "\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m cast(\u001B[38;5;28mint\u001B[39m, _num_cols) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols):\n",
       "\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:432\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
       "\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m    423\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    424\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m    425\u001B[0m     )\n",
       "\u001B[1;32m    427\u001B[0m (\n",
       "\u001B[1;32m    428\u001B[0m     infer_dict_as_struct,\n",
       "\u001B[1;32m    429\u001B[0m     infer_array_from_first_element,\n",
       "\u001B[1;32m    430\u001B[0m     infer_map_from_first_pair,\n",
       "\u001B[1;32m    431\u001B[0m     prefer_timestamp_ntz,\n",
       "\u001B[0;32m--> 432\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mget_configs(\n",
       "\u001B[1;32m    433\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    435\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    436\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.timestampType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    437\u001B[0m )\n",
       "\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mreduce(\n",
       "\u001B[1;32m    439\u001B[0m     _merge_type,\n",
       "\u001B[1;32m    440\u001B[0m     (\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    450\u001B[0m     ),\n",
       "\u001B[1;32m    451\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1819\u001B[0m, in \u001B[0;36mSparkConnectClient.get_configs\u001B[0;34m(self, *keys)\u001B[0m\n",
       "\u001B[1;32m   1817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mkeys: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n",
       "\u001B[1;32m   1818\u001B[0m     op \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGet(keys\u001B[38;5;241m=\u001B[39mkeys))\n",
       "\u001B[0;32m-> 1819\u001B[0m     configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig(op)\u001B[38;5;241m.\u001B[39mpairs)\n",
       "\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(configs\u001B[38;5;241m.\u001B[39mget(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1858\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1858\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n",
       "\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1851\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1849\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n",
       "\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n",
       "\u001B[1;32m   1852\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[1;32m   1853\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n",
       "\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n",
       "\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1853\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[0;32m-> 1853\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    270\u001B[0m     request: Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n\u001B[1;32m    278\u001B[0m         request,\n\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    284\u001B[0m     )\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n\u001B[1;32m    331\u001B[0m )\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n\u001B[1;32m    316\u001B[0m         request,\n\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n\u001B[1;32m    322\u001B[0m     )\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1192\u001B[0m (\n\u001B[1;32m   1193\u001B[0m     state,\n\u001B[1;32m   1194\u001B[0m     call,\n\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001B[1;32m   1197\u001B[0m )\n\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
        "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-07-18T11:29:48.562756459+00:00\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-7903817674957702>, line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m df_7 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, columns)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:657\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    655\u001B[0m         _schema \u001B[38;5;241m=\u001B[39m StructType()\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, _schema)\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 657\u001B[0m     _schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(_data, _cols)\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m cast(\u001B[38;5;28mint\u001B[39m, _num_cols) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols):\n\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:432\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m    423\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    424\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    425\u001B[0m     )\n\u001B[1;32m    427\u001B[0m (\n\u001B[1;32m    428\u001B[0m     infer_dict_as_struct,\n\u001B[1;32m    429\u001B[0m     infer_array_from_first_element,\n\u001B[1;32m    430\u001B[0m     infer_map_from_first_pair,\n\u001B[1;32m    431\u001B[0m     prefer_timestamp_ntz,\n\u001B[0;32m--> 432\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mget_configs(\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.timestampType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    437\u001B[0m )\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mreduce(\n\u001B[1;32m    439\u001B[0m     _merge_type,\n\u001B[1;32m    440\u001B[0m     (\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    450\u001B[0m     ),\n\u001B[1;32m    451\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1819\u001B[0m, in \u001B[0;36mSparkConnectClient.get_configs\u001B[0;34m(self, *keys)\u001B[0m\n\u001B[1;32m   1817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mkeys: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n\u001B[1;32m   1818\u001B[0m     op \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGet(keys\u001B[38;5;241m=\u001B[39mkeys))\n\u001B[0;32m-> 1819\u001B[0m     configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig(op)\u001B[38;5;241m.\u001B[39mpairs)\n\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(configs\u001B[38;5;241m.\u001B[39mget(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1858\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1858\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1851\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1849\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m   1852\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m   1853\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Laptop\", 800), (\"Mouse\", 25), (\"Keyboard\", 150), (\"Monitor\", 300)]\n",
    "columns = [\"product\", \"price\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_7 = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95139ef-fe8a-479d-9921-bf5c5f3fa45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product</th><th>price</th></tr></thead><tbody><tr><td>Laptop</td><td>800</td></tr><tr><td>Mouse</td><td>25</td></tr><tr><td>Keyboard</td><td>150</td></tr><tr><td>Monitor</td><td>300</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Laptop",
         800
        ],
        [
         "Mouse",
         25
        ],
        [
         "Keyboard",
         150
        ],
        [
         "Monitor",
         300
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_7.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d11a16-9dfa-456f-ba0e-c5c65cbf99a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_7=df_7.withColumn(\"Price_catagory\",\n",
    "                     when(col(\"price\")>100,\"low\")\n",
    "                     .when((col(\"price\")>100) & (col(\"price\")<500),'Mddium')\n",
    "                     .otherwise(\"High\")\n",
    "                     \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b28f5d-5955-4734-b811-b98e0a314ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_7.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8133202-1047-49b8-9d17-c0ae07c4a9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 8 \n",
    "Given employee data with columns:\n",
    "employee_id, department, salary, join_date\n",
    "\n",
    "You need to:\n",
    "\n",
    "Calculate cumulative salary within each department, ordered by join_date.\n",
    "\n",
    "Assign dense rank within each department based on salary in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98adc48b-c112-48a6-b151-5982ff35b652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1853\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[0;32m-> 1853\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n",
       "\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    270\u001B[0m     request: Any,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n",
       "\u001B[1;32m    278\u001B[0m         request,\n",
       "\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n",
       "\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n",
       "\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n",
       "\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n",
       "\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m    284\u001B[0m     )\n",
       "\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n",
       "\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n",
       "\u001B[1;32m    331\u001B[0m )\n",
       "\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n",
       "\u001B[1;32m    316\u001B[0m         request,\n",
       "\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n",
       "\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n",
       "\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n",
       "\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n",
       "\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n",
       "\u001B[1;32m    322\u001B[0m     )\n",
       "\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n",
       "\u001B[1;32m   1192\u001B[0m (\n",
       "\u001B[1;32m   1193\u001B[0m     state,\n",
       "\u001B[1;32m   1194\u001B[0m     call,\n",
       "\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n",
       "\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n",
       "\u001B[1;32m   1197\u001B[0m )\n",
       "\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n",
       "\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
       "\n",
       "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-07-18T11:31:29.784852582+00:00\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7903817674957709>, line 19\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memployee_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoin_date\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n",
       "\u001B[0;32m---> 19\u001B[0m df_8 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, columns)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:657\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m    655\u001B[0m         _schema \u001B[38;5;241m=\u001B[39m StructType()\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, _schema)\n",
       "\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 657\u001B[0m     _schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(_data, _cols)\n",
       "\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m cast(\u001B[38;5;28mint\u001B[39m, _num_cols) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols):\n",
       "\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:432\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n",
       "\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
       "\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m    423\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    424\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m    425\u001B[0m     )\n",
       "\u001B[1;32m    427\u001B[0m (\n",
       "\u001B[1;32m    428\u001B[0m     infer_dict_as_struct,\n",
       "\u001B[1;32m    429\u001B[0m     infer_array_from_first_element,\n",
       "\u001B[1;32m    430\u001B[0m     infer_map_from_first_pair,\n",
       "\u001B[1;32m    431\u001B[0m     prefer_timestamp_ntz,\n",
       "\u001B[0;32m--> 432\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mget_configs(\n",
       "\u001B[1;32m    433\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    435\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    436\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.timestampType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    437\u001B[0m )\n",
       "\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mreduce(\n",
       "\u001B[1;32m    439\u001B[0m     _merge_type,\n",
       "\u001B[1;32m    440\u001B[0m     (\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    450\u001B[0m     ),\n",
       "\u001B[1;32m    451\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1819\u001B[0m, in \u001B[0;36mSparkConnectClient.get_configs\u001B[0;34m(self, *keys)\u001B[0m\n",
       "\u001B[1;32m   1817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mkeys: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n",
       "\u001B[1;32m   1818\u001B[0m     op \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGet(keys\u001B[38;5;241m=\u001B[39mkeys))\n",
       "\u001B[0;32m-> 1819\u001B[0m     configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig(op)\u001B[38;5;241m.\u001B[39mpairs)\n",
       "\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(configs\u001B[38;5;241m.\u001B[39mget(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1858\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1858\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n",
       "\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1851\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1849\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n",
       "\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n",
       "\u001B[1;32m   1852\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n",
       "\u001B[1;32m   1853\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n",
       "\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n",
       "\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1853\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1852\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[0;32m-> 1853\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_response_integrity(resp)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    270\u001B[0m     request: Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m     compression: Optional[grpc\u001B[38;5;241m.\u001B[39mCompression] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 277\u001B[0m     response, ignored_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_call(\n\u001B[1;32m    278\u001B[0m         request,\n\u001B[1;32m    279\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    280\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[1;32m    281\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mcredentials,\n\u001B[1;32m    282\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mwait_for_ready,\n\u001B[1;32m    283\u001B[0m         compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    284\u001B[0m     )\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    329\u001B[0m call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interceptor\u001B[38;5;241m.\u001B[39mintercept_unary_unary(\n\u001B[1;32m    330\u001B[0m     continuation, client_call_details, request\n\u001B[1;32m    331\u001B[0m )\n\u001B[0;32m--> 332\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call\u001B[38;5;241m.\u001B[39mresult(), call\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001B[0m, in \u001B[0;36m_InactiveRpcError.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001B[0;34m(new_details, request)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 315\u001B[0m     response, call \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_thunk(new_method)\u001B[38;5;241m.\u001B[39mwith_call(\n\u001B[1;32m    316\u001B[0m         request,\n\u001B[1;32m    317\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mnew_timeout,\n\u001B[1;32m    318\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mnew_metadata,\n\u001B[1;32m    319\u001B[0m         credentials\u001B[38;5;241m=\u001B[39mnew_credentials,\n\u001B[1;32m    320\u001B[0m         wait_for_ready\u001B[38;5;241m=\u001B[39mnew_wait_for_ready,\n\u001B[1;32m    321\u001B[0m         compression\u001B[38;5;241m=\u001B[39mnew_compression,\n\u001B[1;32m    322\u001B[0m     )\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _UnaryOutcome(response, call)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.with_call\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m   1192\u001B[0m (\n\u001B[1;32m   1193\u001B[0m     state,\n\u001B[1;32m   1194\u001B[0m     call,\n\u001B[1;32m   1195\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(\n\u001B[1;32m   1196\u001B[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001B[1;32m   1197\u001B[0m )\n\u001B[0;32m-> 1198\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
        "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-07-18T11:31:29.784852582+00:00\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-7903817674957709>, line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memployee_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoin_date\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Create DataFrame\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m df_8 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, columns)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:657\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    655\u001B[0m         _schema \u001B[38;5;241m=\u001B[39m StructType()\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m, _schema)\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 657\u001B[0m     _schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(_data, _cols)\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _cols \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m cast(\u001B[38;5;28mint\u001B[39m, _num_cols) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols):\n\u001B[1;32m    660\u001B[0m         _num_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(_cols)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:432\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m    423\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    424\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    425\u001B[0m     )\n\u001B[1;32m    427\u001B[0m (\n\u001B[1;32m    428\u001B[0m     infer_dict_as_struct,\n\u001B[1;32m    429\u001B[0m     infer_array_from_first_element,\n\u001B[1;32m    430\u001B[0m     infer_map_from_first_pair,\n\u001B[1;32m    431\u001B[0m     prefer_timestamp_ntz,\n\u001B[0;32m--> 432\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mget_configs(\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.inferNestedDictAsStruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.pyspark.legacy.inferMapTypeFromFirstPair.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.timestampType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    437\u001B[0m )\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m functools\u001B[38;5;241m.\u001B[39mreduce(\n\u001B[1;32m    439\u001B[0m     _merge_type,\n\u001B[1;32m    440\u001B[0m     (\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    450\u001B[0m     ),\n\u001B[1;32m    451\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1819\u001B[0m, in \u001B[0;36mSparkConnectClient.get_configs\u001B[0;34m(self, *keys)\u001B[0m\n\u001B[1;32m   1817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mkeys: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n\u001B[1;32m   1818\u001B[0m     op \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(get\u001B[38;5;241m=\u001B[39mpb2\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mGet(keys\u001B[38;5;241m=\u001B[39mkeys))\n\u001B[0;32m-> 1819\u001B[0m     configs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig(op)\u001B[38;5;241m.\u001B[39mpairs)\n\u001B[1;32m   1820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(configs\u001B[38;5;241m.\u001B[39mget(key) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m keys)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1858\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1858\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\u001B[1;32m   2050\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[1;32m   2051\u001B[0m                 error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNO_ACTIVE_SESSION\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m   2052\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1851\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1849\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m   1852\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m   1853\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_done:\n\u001B[0;32m--> 295\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait()\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001B[0m, in \u001B[0;36mRetrying._wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;66;03m# Exceeded retries\u001B[39;00m\n\u001B[1;32m    279\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGiven up on retrying. error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(exception)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m RetriesExceeded(error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETRIES_EXCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{}) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexception\u001B[39;00m\n",
        "\u001B[0;31mRetriesExceeded\u001B[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"HR\", 50000, \"2022-01-01\"),\n",
    "    (2, \"IT\", 70000, \"2021-06-15\"),\n",
    "    (3, \"HR\", 60000, \"2023-03-10\"),\n",
    "    (4, \"IT\", 80000, \"2020-12-01\")\n",
    "]\n",
    "columns = [\"employee_id\", \"department\", \"salary\", \"join_date\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_8 = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d63ba1d8-0e69-4534-937f-bb9ec91539e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LinkedIn_Questions_From_Start-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}