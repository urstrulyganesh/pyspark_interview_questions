{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da0d4fc-4d5f-4834-91e2-778c643ebe8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf8f855-93bc-4be9-9782-b91e8b8bd7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 1: Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bee250-6d44-42f3-8ec1-9685553b9dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n|product_id|low_fats|recyclable|\n+----------+--------+----------+\n|         0|       Y|         N|\n|         1|       Y|         Y|\n|         2|       N|         Y|\n|         3|       Y|         Y|\n|         4|       N|         N|\n+----------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (0, 'Y', 'N'),\n",
    "    (1, 'Y', 'Y'),\n",
    "    (2, 'N', 'Y'),\n",
    "    (3, 'Y', 'Y'),\n",
    "    (4, 'N', 'N')\n",
    "]\n",
    "\n",
    "schema1 = ['product_id', 'low_fats', 'recyclable']\n",
    "\n",
    "# Write a solution to find the ids of products that are both low fat and recyclable.\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7513cb1a-5462-48a7-bff7-091a894d5771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|product_id|\n+----------+\n|         1|\n|         3|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 1: SPARK\n",
    "df1.select(\"product_id\").filter((col(\"low_fats\")=='Y') & (col(\"recyclable\")=='Y')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e8c755-74db-4a79-a2bb-bb35710c6e7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|product_id|\n+----------+\n|         1|\n|         3|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 1: SQL\n",
    "df1.createOrReplaceTempView(\"df1_tbl1\")\n",
    "spark.sql('''\n",
    "          select product_id from df1_tbl1 where low_fats='Y' and recyclable='Y'\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6883cc1-3119-4d38-9de6-18f910b97b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 2: Find Customer Referee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d98851-05b8-489a-8004-f85f725b8351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n| id|name|referee_id|\n+---+----+----------+\n|  1|Will|      null|\n|  2|Jane|      null|\n|  3|Alex|         2|\n|  4|Bill|      null|\n|  5|Zack|         1|\n|  6|Mark|         2|\n+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data2 = [\n",
    "    (1, 'Will', None),\n",
    "    (2, 'Jane', None),\n",
    "    (3, 'Alex', 2),\n",
    "    (4, 'Bill', None),\n",
    "    (5, 'Zack', 1),\n",
    "    (6, 'Mark', 2)\n",
    "]\n",
    "\n",
    "schema2 = ['id', 'name', 'referee_id']\n",
    "\n",
    "# Find the names of the customer that are not referred by the customer with id = 2.\n",
    "df2 = spark.createDataFrame(data = data2, schema= schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428e82b1-c566-4204-b8bc-946dd61f9621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n|Will|\n|Jane|\n|Bill|\n|Zack|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 2: SPARK\n",
    "\n",
    "df2.select(\"name\")\\\n",
    "    .filter((col(\"referee_id\")!=2) | (col(\"referee_id\").isNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d7233d-2d54-4d4b-9ce6-3ec1fd04dc58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n|Will|\n|Jane|\n|Bill|\n|Zack|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 2: SQL\n",
    "\n",
    "df2.createOrReplaceTempView(\"df2_tbl2\")\n",
    "spark.sql('''\n",
    "          select name from df2_tbl2 where referee_id!=2 or referee_id IS NULL\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e525023-5473-4530-954a-4812c34cee98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 3: Big Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672e25ec-cbbf-4c67-8e7a-67f1cde854e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+----------+------------+\n|       name|continent|   area|population|         gdp|\n+-----------+---------+-------+----------+------------+\n|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n|    Albania|   Europe|  28748|   2831741| 12960000000|\n|    Algeria|   Africa|2381741|  37100000|188681000000|\n|    Andorra|   Europe|    468|     78115|  3712000000|\n|     Angola|   Africa|1246700|  20609294|100990000000|\n+-----------+---------+-------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "data3 = [\n",
    "    ('Afghanistan', 'Asia', 652230, 25500100, 20343000000),\n",
    "    ('Albania', 'Europe', 28748, 2831741, 12960000000),\n",
    "    ('Algeria', 'Africa', 2381741, 37100000, 188681000000),\n",
    "    ('Andorra', 'Europe', 468, 78115, 3712000000),\n",
    "    ('Angola', 'Africa', 1246700, 20609294, 100990000000)\n",
    "]\n",
    "\n",
    "schema3 = ['name', 'continent', 'area', 'population', 'gdp']\n",
    "\n",
    "# Write a solution to find the name, population, and area of the big countries.\n",
    "# A country is big if:\n",
    "# it has an area of at least three million (i.e., 3000000 km2), or\n",
    "# it has a population of at least twenty-five million (i.e., 25000000).\n",
    "\n",
    "df3= spark.createDataFrame(data= data3, schema= schema3)\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03709552-c71c-49fd-ae8e-29c659b5f438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n|       name|population|   area|\n+-----------+----------+-------+\n|Afghanistan|  25500100| 652230|\n|    Algeria|  37100000|2381741|\n+-----------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 3: SPARK\n",
    "\n",
    "df3.select(\"name\", \"population\", \"area\")\\\n",
    "    .filter((col(\"area\")>=3000000) | (col(\"population\")>=25000000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a14bdb-b5b4-49bf-bcfc-8896ca497d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+----------+------------+\n|       name|continent|   area|population|         gdp|\n+-----------+---------+-------+----------+------------+\n|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n|    Algeria|   Africa|2381741|  37100000|188681000000|\n+-----------+---------+-------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 3: SQL\n",
    "\n",
    "df3.createOrReplaceTempView(\"df3_tbl3\")\n",
    "spark.sql('''\n",
    "          select * from df3_tbl3 where area>= 3000000 or population>= 25000000\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4c6a1bd-943a-4351-b23b-b9e8dc4970e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 4: Article Views I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf02ccd-2ae3-4139-a794-2a1b1abebec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+----------+\n|article_id|author_id|viewer_id| view_date|\n+----------+---------+---------+----------+\n|         1|        3|        5|2019-08-01|\n|         1|        3|        6|2019-08-02|\n|         2|        7|        7|2019-08-01|\n|         2|        7|        6|2019-08-02|\n|         4|        7|        1|2019-07-22|\n|         3|        4|        4|2019-07-21|\n|         3|        4|        4|2019-07-21|\n+----------+---------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data4 = [\n",
    "    (1, 3, 5, '2019-08-01'),\n",
    "    (1, 3, 6, '2019-08-02'),\n",
    "    (2, 7, 7, '2019-08-01'),\n",
    "    (2, 7, 6, '2019-08-02'),\n",
    "    (4, 7, 1, '2019-07-22'),\n",
    "    (3, 4, 4, '2019-07-21'),\n",
    "    (3, 4, 4, '2019-07-21')\n",
    "]\n",
    "\n",
    "schema4 = ['article_id', 'author_id', 'viewer_id', 'view_date']\n",
    "\n",
    "# Write a solution to find all the authors that viewed at least one of their own articles.\n",
    "# Return the result table sorted by id in ascending order.\n",
    "\n",
    "df4= spark.createDataFrame(data= data4, schema= schema4)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476e2d39-3ac8-4688-8895-76430f2af829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|author_id|\n+---------+\n|        4|\n|        7|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 4: SPARK\n",
    "\n",
    "df4.select(\"author_id\").distinct()\\\n",
    "    .filter(col(\"author_id\")==col(\"viewer_id\"))\\\n",
    "        .sort(col(\"author_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c60f049-00bd-4559-9eb4-2f5f136437c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|author_id|\n+---------+\n|        4|\n|        7|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 4: SQL\n",
    "\n",
    "df4.createOrReplaceTempView(\"df4_tbl\")\n",
    "spark.sql('''\n",
    "          select distinct(author_id) from df4_tbl where author_id=viewer_id\n",
    "          order by author_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a1a889-040f-4fe6-907d-458d81159f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 5: Invalid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4924500-12d1-4d3e-9f70-aebac5af6e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n|tweet_id|             content|\n+--------+--------------------+\n|       1|         Let us Code|\n|       2|More than fifteen...|\n+--------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data5 = [\n",
    "    (1, \"Let us Code\"),\n",
    "    (2, \"More than fifteen chars are here!\")\n",
    "]\n",
    "\n",
    "schema5 = [\"tweet_id\", \"content\"]\n",
    "\n",
    "# Write a solution to find the IDs of the invalid tweets.\n",
    "# The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "df5 = spark.createDataFrame(data = data5, schema = schema5)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdeb8fee-0e9b-4dd3-99aa-8f50369b5c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|tweet_id|\n+--------+\n|       2|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 5: SPARK\n",
    "df5.select(col(\"tweet_id\"))\\\n",
    "    .filter(length(col(\"content\"))>15).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806b218e-9da6-4d69-a8e3-a1a9b4f3e958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|tweet_id|\n+--------+\n|       2|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 5: SQL\n",
    "df5.createOrReplaceTempView(\"df5_tbl\")\n",
    "spark.sql('''\n",
    "          select tweet_id from df5_tbl where len(content)>15\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8deaddb2-2ea5-4214-aa7b-3f410eca387b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 6: Replace Employee ID With The Unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b460347-1913-4a99-ab84-b7751dda205e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|    name|\n+---+--------+\n|  1|   Alice|\n|  7|     Bob|\n| 11|    Meir|\n| 90| Winston|\n|  3|Jonathan|\n+---+--------+\n\n+---+---------+\n| id|unique_id|\n+---+---------+\n|  3|        1|\n| 11|        2|\n| 90|        3|\n+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "data6_emp = [\n",
    "    (1, \"Alice\"),\n",
    "    (7, \"Bob\"),\n",
    "    (11, \"Meir\"),\n",
    "    (90, \"Winston\"),\n",
    "    (3, \"Jonathan\")\n",
    "]\n",
    "schema6_emp = [\"id\", \"name\"]\n",
    "\n",
    "data6_empUNI = [\n",
    "    (3, 1),\n",
    "    (11, 2),\n",
    "    (90, 3)\n",
    "]\n",
    "schema6_empUNI = [\"id\", \"unique_id\"]\n",
    "\n",
    "# Write a solution to show the unique ID, name of each user, If a user does not have a unique ID replace just show null.\n",
    "\n",
    "df6_emp = spark.createDataFrame(data= data6_emp, schema =schema6_emp)\n",
    "df6_empUNI = spark.createDataFrame(data= data6_empUNI, schema= schema6_empUNI)\n",
    "\n",
    "df6_emp.show()\n",
    "df6_empUNI.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4d265d-ea7d-4b98-96a3-82b2cdc29c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n|    name|unique_id|\n+--------+---------+\n|   Alice|     null|\n|     Bob|     null|\n|    Meir|        2|\n| Winston|        3|\n|Jonathan|        1|\n+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 6: SPARK\n",
    "\n",
    "df6_emp.join(df6_empUNI, df6_emp[\"id\"]==df6_empUNI[\"id\"], \"left\")\\\n",
    "    .select(df6_emp[\"name\"], df6_empUNI[\"unique_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3de48d-0f21-4bb4-8d88-9b94992fc4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n|    name|unique_id|\n+--------+---------+\n|   Alice|     null|\n|     Bob|     null|\n|    Meir|        2|\n| Winston|        3|\n|Jonathan|        1|\n+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 6: SQL\n",
    "df6_emp.createOrReplaceTempView(\"df6_emp_tbl\")\n",
    "df6_empUNI.createOrReplaceTempView(\"df6_empUNI_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select df6_emp_tbl.name, df6_empUNI_tbl.unique_id from df6_emp_tbl LEFT JOIN df6_empUNI_tbl ON df6_emp_tbl.id = df6_empUNI_tbl.id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f8764a-c2ca-4516-95db-a512734065b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 7: Product Sales Analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefd969a-a37d-4017-9e63-84787e34f17f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n|sale_id|product_id|year|quantity|price|\n+-------+----------+----+--------+-----+\n|      1|       100|2008|      10| 5000|\n|      2|       100|2009|      12| 5000|\n|      7|       200|2011|      15| 9000|\n+-------+----------+----+--------+-----+\n\n+----------+------------+\n|product_id|product_name|\n+----------+------------+\n|       100|       Nokia|\n|       200|       Apple|\n|       300|     Samsung|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df7_sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "df7_sales_data_schema = ['sale_id', 'product_id', 'year', 'quantity', 'price']\n",
    "\n",
    "df7_product_data = [\n",
    "    (100, 'Nokia'),\n",
    "    (200, 'Apple'),\n",
    "    (300, 'Samsung')\n",
    "]\n",
    "df7_product_schema = ['product_id', 'product_name']\n",
    "\n",
    "# Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "df7_sales= spark.createDataFrame(data= df7_sales_data, schema= df7_sales_data_schema)\n",
    "df7_product= spark.createDataFrame(data= df7_product_data, schema= df7_product_schema)\n",
    "\n",
    "df7_sales.show()\n",
    "df7_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f12ed2-ed80-4525-8284-c65d169f0a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n|product_name|year|price|\n+------------+----+-----+\n|       Nokia|2008| 5000|\n|       Nokia|2009| 5000|\n|       Apple|2011| 9000|\n+------------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 7: SPARK\n",
    "df7_sales.join(df7_product, df7_sales[\"product_id\"]==df7_product[\"product_id\"], \"inner\")\\\n",
    "    .select(df7_product[\"product_name\"], df7_sales[\"year\"], df7_sales[\"price\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67353785-938a-4918-bc9c-e0db00f6e755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n|product_name|year|price|\n+------------+----+-----+\n|       Nokia|2008| 5000|\n|       Nokia|2009| 5000|\n|       Apple|2011| 9000|\n+------------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 7: SQL\n",
    "df7_product.createOrReplaceTempView(\"df7_product_tbl\")\n",
    "df7_sales.createOrReplaceTempView(\"df7_sales_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select product.product_name, sales.year, sales.price\n",
    "          from df7_product_tbl as product inner join df7_sales_tbl as sales \n",
    "          on product.product_id= sales.product_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ac861df-f641-4b59-b281-eb05b37090ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 8: Customer Who Visited but Did Not Make Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b8b711-e22c-4e8c-9b74-e240b37d3951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|visit_id|customer_id|\n+--------+-----------+\n|       1|         23|\n|       2|          9|\n|       4|         30|\n|       5|         54|\n|       6|         96|\n|       7|         54|\n|       8|         54|\n+--------+-----------+\n\n+--------------+--------+------+\n|transaction_id|visit_id|amount|\n+--------------+--------+------+\n|             2|       5|   310|\n|             3|       5|   300|\n|             9|       5|   200|\n|            12|       1|   910|\n|            13|       2|   970|\n+--------------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df8_visits_data = [\n",
    "    (1, 23),\n",
    "    (2, 9),\n",
    "    (4, 30),\n",
    "    (5, 54),\n",
    "    (6, 96),\n",
    "    (7, 54),\n",
    "    (8, 54)\n",
    "]\n",
    "df8_visits_schema = ['visit_id', 'customer_id']\n",
    "\n",
    "df8_transactions_data = [\n",
    "    (2, 5, 310),\n",
    "    (3, 5, 300),\n",
    "    (9, 5, 200),\n",
    "    (12, 1, 910),\n",
    "    (13, 2, 970)\n",
    "]\n",
    "df8_transactions_schema = ['transaction_id', 'visit_id', 'amount']\n",
    "\n",
    "# Write a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.\n",
    "\n",
    "df8_visits= spark.createDataFrame(data= df8_visits_data, schema= df8_visits_schema)\n",
    "df8_transactions= spark.createDataFrame(data= df8_transactions_data, schema= df8_transactions_schema)\n",
    "\n",
    "df8_visits.show()\n",
    "df8_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf81a6c3-743d-486d-90d1-434930db6912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n|customer_id|count(visit_id)|\n+-----------+---------------+\n|         30|              1|\n|         96|              1|\n|         54|              2|\n+-----------+---------------+\n\n+-----------+--------------+\n|customer_id|count_no_trans|\n+-----------+--------------+\n|         30|             1|\n|         96|             1|\n|         54|             2|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 8: SPARK\n",
    "\n",
    "df8_visits.join(df8_transactions, df8_visits[\"visit_id\"] == df8_transactions[\"visit_id\"], \"left_anti\")\\\n",
    "    .groupBy(df8_visits[\"customer_id\"]).agg(count(df8_visits[\"visit_id\"])).alias(\"count_no_trans\").show()\n",
    "\n",
    "df8_visits.join(df8_transactions, df8_visits[\"visit_id\"] == df8_transactions[\"visit_id\"], \"left_anti\")\\\n",
    "    .groupBy(df8_visits[\"customer_id\"]).agg(count(df8_visits[\"visit_id\"]).alias(\"count_no_trans\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc487ac-f84c-40ca-a33a-9c329be56da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n|customer_id|count_no_trans|\n+-----------+--------------+\n|         30|             1|\n|         96|             1|\n|         54|             2|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 8: SQL\n",
    "\n",
    "df8_transactions.createOrReplaceTempView(\"df8_transactions_tbl\")\n",
    "df8_visits.createOrReplaceTempView(\"df8_visits_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select visits.customer_id, count(visits.visit_id) as count_no_trans from df8_visits_tbl as visits left join df8_transactions_tbl as transaction on transaction.visit_id=visits.visit_id\n",
    "          where transaction.transaction_id IS NULL\n",
    "          group by visits.customer_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdab92f1-231d-4f39-89a9-20379c56f5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 9: Rising Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a8c53e-9dbe-4763-9af0-31095911abb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n| id|recordDate|temperature|\n+---+----------+-----------+\n|  1|2015-01-01|         10|\n|  2|2015-01-02|         25|\n|  3|2015-01-03|         20|\n|  4|2015-01-04|         30|\n+---+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df9_weather_data = [\n",
    "    (1, '2015-01-01', 10),\n",
    "    (2, '2015-01-02', 25),\n",
    "    (3, '2015-01-03', 20),\n",
    "    (4, '2015-01-04', 30)\n",
    "]\n",
    "df9_weather_schema = ['id', 'recordDate', 'temperature']\n",
    "\n",
    "# Write a solution to find all dates' id with higher temperatures compared to its previous dates (yesterday).\n",
    "\n",
    "df9= spark.createDataFrame(data= df9_weather_data, schema= df9_weather_schema)\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceca364b-0c1b-4039-9e7b-ec73c636e2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1978085944061911>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# QUESTION 9: SPARK\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m df9\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjoin(df9\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[43ma\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecordDate\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m date_sub(b[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecordDate\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'a' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1978085944061911>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# QUESTION 9: SPARK\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df9\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjoin(df9\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[43ma\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecordDate\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m date_sub(b[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecordDate\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'a' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'a' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QUESTION 9: SPARK\n",
    "\n",
    "df9.alias(\"a\").join(df9.alias(\"b\"), a[\"recordDate\"] == date_sub(b[\"recordDate\"], 1), \"inner\").show()\n",
    "\n",
    "# because a and b are not defined variables. In PySpark, alias() creates a table alias inside the DataFrame, but it doesn't define Python variables named a or b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e128005-efa5-4b8b-9850-f398c90a7d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  2|\n|  4|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df9.alias(\"a\").join(df9.alias(\"b\"), (datediff(col(\"a.recordDate\"), col(\"b.recordDate\"))==1) & (col(\"a.temperature\")> col(\"b.temperature\")), \"inner\")\\\n",
    "    .select(col(\"a.id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf6a31f-7021-4137-86f3-14d89cf78d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  2|\n|  4|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 9: SPARK\n",
    "\n",
    "df9.alias(\"a\").join(df9.alias(\"b\"), (col(\"a.recordDate\") == date_sub(col(\"b.recordDate\"), 1)) & (col(\"b.temperature\")>col(\"a.temperature\")), \"inner\")\\\n",
    "    .select(col(\"b.id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447d7589-26e1-4e14-bd69-df65d4a80736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  2|\n|  4|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 9: SPARK USING WINDOWS FUNCTION\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Ensure recordDate is of DateType\n",
    "df9.withColumn(\"recordDate\", to_date(col(\"recordDate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "win=Window.orderBy(col(\"recordDate\"))\n",
    "\n",
    "df9.withColumn(\"prev_temp\", lag(col(\"temperature\")).over(win))\\\n",
    "    .filter(col(\"temperature\")>col(\"prev_temp\"))\\\n",
    "        .select(col(\"id\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe709fb-6964-4db7-8607-e95ad0ad3c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  2|\n|  4|\n+---+\n\n+---+\n| id|\n+---+\n|  2|\n|  4|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 9: SQL\n",
    "\n",
    "df9.createOrReplaceTempView(\"df9_tbl\")\n",
    "spark.sql('''\n",
    "          select b.id from df9_tbl a inner join df9_tbl b on a.recordDate = date_sub(b.recordDate, 1) and b.temperature>a.temperature\n",
    "          ''').show()\n",
    "\n",
    "# USING WINDOW FUNCTION\n",
    "spark.sql('''\n",
    "          select id from(\n",
    "          select *, lag(temperature, 1) over(order by recordDate) as prev_temp from df9_tbl) a\n",
    "          where a.temperature>a.prev_temp\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e275b2c7-7d46-4fcc-90e2-f1c8bee5dfb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 10: Average Time of Process per Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eae81d0-6035-4462-befd-3c2a880dd430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+---------+\n|machine_id|process_id|activity_type|timestamp|\n+----------+----------+-------------+---------+\n|         0|         0|        start|    0.712|\n|         0|         0|          end|     1.52|\n|         0|         1|        start|     3.14|\n|         0|         1|          end|     4.12|\n|         1|         0|        start|     0.55|\n|         1|         0|          end|     1.55|\n|         1|         1|        start|     0.43|\n|         1|         1|          end|     1.42|\n|         2|         0|        start|      4.1|\n|         2|         0|          end|    4.512|\n|         2|         1|        start|      2.5|\n|         2|         1|          end|      5.0|\n+----------+----------+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df10_activity_data = [\n",
    "    (0, 0, 'start', 0.712),\n",
    "    (0, 0, 'end', 1.520),\n",
    "    (0, 1, 'start', 3.140),\n",
    "    (0, 1, 'end', 4.120),\n",
    "    (1, 0, 'start', 0.550),\n",
    "    (1, 0, 'end', 1.550),\n",
    "    (1, 1, 'start', 0.430),\n",
    "    (1, 1, 'end', 1.420),\n",
    "    (2, 0, 'start', 4.100),\n",
    "    (2, 0, 'end', 4.512),\n",
    "    (2, 1, 'start', 2.500),\n",
    "    (2, 1, 'end', 5.000)\n",
    "]\n",
    "df10_activity_schema = ['machine_id', 'process_id', 'activity_type', 'timestamp']\n",
    "\n",
    "# There is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n",
    "# The time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n",
    "# The resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.\n",
    "\n",
    "df10= spark.createDataFrame(data= df10_activity_data, schema=df10_activity_schema)\n",
    "\n",
    "df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613e6efc-1bce-49e6-a9c2-0cec1bb8b610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         0|          0.894|\n|         1|          0.995|\n|         2|          1.456|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# After the join, both DataFrames have columns with the same names, like machine_id, process_id, and timestamp. PySpark automatically renames the duplicate column names with suffixes like _left, _right (or more specifically, it uses the aliases of the DataFrames if provided). So when you do: (df10_end[\"timestamp\"]-df10_start[\"timestamp\"]), PySpark may not know which timestamp you mean, or you'll get an ambiguous column error or incorrect column reference.\n",
    "\n",
    "# df10_start = df10.filter(col(\"activity_type\")==\"start\")\n",
    "# df10_end = df10.filter(col(\"activity_type\")==\"end\")\n",
    "\n",
    "# df10_start.join(df10_end, (df10_start[\"machine_id\"]==df10_end[\"machine_id\"]) & (df10_start[\"process_id\"]==df10_end[\"process_id\"]))\\\n",
    "    # .withColumn(\"process_time\", (df10_end[\"timestamp\"]-df10_start[\"timestamp\"]))\\\n",
    "        # .show()\n",
    "\n",
    "df10_start = df10.filter(col(\"activity_type\")==\"start\").alias(\"start\")\n",
    "df10_end = df10.filter(col(\"activity_type\")==\"end\").alias(\"end\")\n",
    "\n",
    "df10_start.join(df10_end, (df10_start[\"machine_id\"]==df10_end[\"machine_id\"]) & (df10_start[\"process_id\"]==df10_end[\"process_id\"]))\\\n",
    "     .withColumn(\"process_time\", (col(\"end.timestamp\")-col(\"start.timestamp\")))\\\n",
    "         .groupBy(col(\"start.machine_id\")).agg(round(avg(col(\"process_time\")),3).alias(\"processing_time\"))\\\n",
    "             .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f2945c-283f-491f-9495-79e81c45925c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         0|          0.894|\n|         1|          0.995|\n|         2|          1.456|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 10: SPARK\n",
    "\n",
    "df10.alias(\"t1\").join(df10.alias(\"t2\"), (col(\"t1.machine_id\") == col(\"t2.machine_id\")) & (col(\"t1.process_id\") == col(\"t2.process_id\")) & (col(\"t1.activity_type\") == \"end\") & (col(\"t2.activity_type\") == \"start\"), \"inner\")\\\n",
    "    .withColumn(\"time_stamp\",(col(\"t1.timestamp\")-col(\"t2.timestamp\")))\\\n",
    "    .groupBy(col(\"t1.machine_id\")).agg(round(avg(col(\"time_stamp\")),3).alias(\"processing_time\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b423a16f-76c5-4423-a793-97bf15f7050c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         0|          0.894|\n|         1|          0.995|\n|         2|          1.456|\n+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 10: SQL\n",
    "\n",
    "df10.createOrReplaceTempView(\"df10_tbl\")\n",
    "spark.sql('''\n",
    "          select t1.machine_id, round(avg(t1.timestamp-t2.timestamp),3) as processing_time \n",
    "from df10_tbl t1 inner join df10_tbl t2 on t1.machine_id=t2.machine_id and t1.process_id=t2.process_id and t1.activity_type=\"end\" and t2.activity_type=\"start\"\n",
    "group by t1.machine_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d320d19f-ea83-4cdb-8078-dfa79a5d4748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 11: Employee Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a664e8-b5f6-4031-a145-62876a56e881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------+\n|empId|  name|supervisor|salary|\n+-----+------+----------+------+\n|    3|  Brad|      null|  4000|\n|    1|  John|         3|  1000|\n|    2|   Dan|         3|  2000|\n|    4|Thomas|         3|  4000|\n+-----+------+----------+------+\n\n+-----+-----+\n|empId|bonus|\n+-----+-----+\n|    2|  500|\n|    4| 2000|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df11_employee_data = [\n",
    "    (3, 'Brad', None, 4000),\n",
    "    (1, 'John', 3, 1000),\n",
    "    (2, 'Dan', 3, 2000),\n",
    "    (4, 'Thomas', 3, 4000)\n",
    "]\n",
    "df11_employee_schema = ['empId', 'name', 'supervisor', 'salary']\n",
    "\n",
    "df11_bonus_data = [\n",
    "    (2, 500),\n",
    "    (4, 2000)\n",
    "]\n",
    "df11_bonus_schema = ['empId', 'bonus']\n",
    "\n",
    "# Write a solution to report the name and bonus amount of each employee with a bonus less than 1000.\n",
    "\n",
    "df11_employee= spark.createDataFrame(data= df11_employee_data, schema=df11_employee_schema)\n",
    "df11_bonus= spark.createDataFrame(data=df11_bonus_data, schema=df11_bonus_schema)\n",
    "\n",
    "df11_employee.show()\n",
    "df11_bonus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3ebbbd-d477-4d04-a78a-4d6815659651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n|name|bonus|\n+----+-----+\n|Brad| null|\n|John| null|\n| Dan|  500|\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 11: SPARK\n",
    "df11_employee.join(df11_bonus, df11_employee[\"empId\"] == df11_bonus[\"empId\"], \"left\")\\\n",
    "    .filter((df11_bonus[\"bonus\"]<1000) | (df11_bonus[\"bonus\"].isNull()))\\\n",
    "        .select(df11_employee[\"name\"], df11_bonus[\"bonus\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358b63ac-bf76-4dee-8b5b-9f47433cf58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n|name|bonus|\n+----+-----+\n|Brad| null|\n|John| null|\n| Dan|  500|\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 11: SQL\n",
    "\n",
    "df11_employee.createOrReplaceTempView(\"df11_emp_tbl\")\n",
    "df11_bonus.createOrReplaceTempView(\"df_bonus_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select e.name, b.bonus from df11_emp_tbl e left join df_bonus_tbl b on e.empId=b.empId \n",
    "          where (b.bonus<1000 or b.bonus IS NULL)\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21d2fa0-6271-4e03-862d-9952fd8eb741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 12: Students and Examinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94ca480-8c54-4d08-9355-c4cfcc6b4681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|student_id|student_name|\n+----------+------------+\n|         1|       Alice|\n|         2|         Bob|\n|        13|        John|\n|         6|        Alex|\n+----------+------------+\n\n+------------+\n|subject_name|\n+------------+\n|        Math|\n|     Physics|\n| Programming|\n+------------+\n\n+----------+------------+\n|student_id|subject_name|\n+----------+------------+\n|         1|        Math|\n|         1|     Physics|\n|         1| Programming|\n|         2| Programming|\n|         1|     Physics|\n|         1|        Math|\n|        13|        Math|\n|        13| Programming|\n|        13|     Physics|\n|         2|        Math|\n|         1|        Math|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df12_students_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (13, \"John\"),\n",
    "    (6, \"Alex\")\n",
    "]\n",
    "df12_students_schema = [\"student_id\", \"student_name\"]\n",
    "\n",
    "df12_subjects_data = [\n",
    "    (\"Math\",),\n",
    "    (\"Physics\",),\n",
    "    (\"Programming\",)\n",
    "]\n",
    "df12_subjects_schema = [\"subject_name\"]\n",
    "\n",
    "df12_examinations_data = [\n",
    "    (1, \"Math\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Programming\"),\n",
    "    (2, \"Programming\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Math\"),\n",
    "    (13, \"Math\"),\n",
    "    (13, \"Programming\"),\n",
    "    (13, \"Physics\"),\n",
    "    (2, \"Math\"),\n",
    "    (1, \"Math\")\n",
    "]\n",
    "df12_examinations_schema = [\"student_id\", \"subject_name\"]\n",
    "\n",
    "# Write a solution to find the number of times each student attended each exam.\n",
    "# Return the result table ordered by student_id and subject_name.\n",
    "# The result table should contain all students and all subjects.\n",
    "\n",
    "df12_students = spark.createDataFrame(df12_students_data, df12_students_schema)\n",
    "df12_subjects = spark.createDataFrame(df12_subjects_data, df12_subjects_schema)\n",
    "df12_exams = spark.createDataFrame(df12_examinations_data, df12_examinations_schema)\n",
    "\n",
    "df12_students.show()\n",
    "df12_subjects.show()\n",
    "df12_exams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a33d1c-a5e8-4ae3-8fa0-eb9a215b0d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n|student_id|student_name|subject_name|attended_exams|\n+----------+------------+------------+--------------+\n|         1|       Alice|        Math|             3|\n|         1|       Alice|     Physics|             2|\n|         1|       Alice| Programming|             1|\n|         2|         Bob|        Math|             1|\n|         2|         Bob|     Physics|             0|\n|         2|         Bob| Programming|             1|\n|         6|        Alex|        Math|             0|\n|         6|        Alex|     Physics|             0|\n|         6|        Alex| Programming|             0|\n|        13|        John|        Math|             1|\n|        13|        John|     Physics|             1|\n|        13|        John| Programming|             1|\n+----------+------------+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 12: SPARK\n",
    "\n",
    "df12_students.crossJoin(df12_subjects).alias(\"df12_ss\")\\\n",
    "    .join(df12_exams, (col(\"df12_ss.student_id\") == df12_exams[\"student_id\"]) & (col(\"df12_ss.subject_name\") == df12_exams[\"subject_name\"]), \"left\")\\\n",
    "        .groupBy(\"df12_ss.student_id\", \"df12_ss.student_name\", \"df12_ss.subject_name\").agg(sum(when (df12_exams[\"student_id\"].isNull(), 0).otherwise(1)).alias(\"attended_exams\"))\\\n",
    "            .orderBy(\"df12_ss.student_id\", \"df12_ss.subject_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d410d016-7c9b-485a-b0aa-a62fc908dd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n|student_id|student_name|subject_name|attended_exams|\n+----------+------------+------------+--------------+\n|         1|       Alice|        Math|             3|\n|         1|       Alice|     Physics|             2|\n|         1|       Alice| Programming|             1|\n|         2|         Bob|        Math|             1|\n|         2|         Bob|     Physics|             0|\n|         2|         Bob| Programming|             1|\n|         6|        Alex|        Math|             0|\n|         6|        Alex|     Physics|             0|\n|         6|        Alex| Programming|             0|\n|        13|        John|        Math|             1|\n|        13|        John|     Physics|             1|\n|        13|        John| Programming|             1|\n+----------+------------+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 12: SQL\n",
    "\n",
    "df12_students.createOrReplaceTempView(\"df12_students_tbl\")\n",
    "df12_subjects.createOrReplaceTempView(\"df12_subjects_tbl\")\n",
    "df12_exams.createOrReplaceTempView(\"df12_exams_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select a.student_id, a.student_name,a.subject_name , sum (case when b.student_id IS NULL then 0 else 1 end) as attended_exams\n",
    "          from (select * from df12_students_tbl, df12_subjects_tbl) as a left join df12_exams_tbl as b on a.student_id=b.student_id and a.subject_name=b.subject_name\n",
    "          group by a.student_id, a.student_name,a.subject_name\n",
    "          order by a.student_id, a.subject_name\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a0e2fe-9536-4e0d-a8a5-56a1bfdbb466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 13: Managers with at Least 5 Direct Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd4efe9-5e5c-49e8-8584-ced35a42bfd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n| id| name|department|managerId|\n+---+-----+----------+---------+\n|101| John|         A|     null|\n|102|  Dan|         A|      101|\n|103|James|         A|      101|\n|104|  Amy|         A|      101|\n|105| Anne|         A|      101|\n|106|  Ron|         B|      101|\n+---+-----+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df13_employee_data = [\n",
    "    (101, \"John\", \"A\", None),\n",
    "    (102, \"Dan\", \"A\", 101),\n",
    "    (103, \"James\", \"A\", 101),\n",
    "    (104, \"Amy\", \"A\", 101),\n",
    "    (105, \"Anne\", \"A\", 101),\n",
    "    (106, \"Ron\", \"B\", 101)\n",
    "]\n",
    "df13_employee_schema = [\"id\", \"name\", \"department\", \"managerId\"]\n",
    "\n",
    "# Write a solution to find managers with at least five direct reports.\n",
    "\n",
    "df13 = spark.createDataFrame(data = df13_employee_data, schema = df13_employee_schema)\n",
    "df13.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d560dcf-f174-4e43-a6b0-6562576f3a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n|John|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 13: SPARK\n",
    "\n",
    "df13.alias(\"df13_a\").join(df13.alias(\"df13_b\"), (col(\"df13_a.id\") == col(\"df13_b.managerId\")), \"inner\")\\\n",
    "    .groupBy(\"df13_a.id\", \"df13_a.name\").agg(count(\"df13_a.id\").alias(\"count_manager\"))\\\n",
    "        .filter(col(\"count_manager\")>=5)\\\n",
    "            .select(\"df13_a.name\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28ae355-6dd1-415c-9a53-0de8dbfbc61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n|John|\n+----+\n\n+----+\n|name|\n+----+\n|John|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 13: SQL\n",
    "\n",
    "df13.createOrReplaceTempView(\"df13_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select name from df13_tbl where id IN( \n",
    "        select managerId from df13_tbl group by managerId having count(*)>=5);\n",
    "          ''').show()\n",
    "\n",
    "spark.sql('''\n",
    "        select a.name from df13_tbl a inner join df13_tbl b on a.id=b.managerId\n",
    "        group by a.id, a.name having  count(a.id)>=5;\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17c2847b-df7d-468c-958d-669706bb9bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 14: Confirmation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e65b7f-6ce1-4270-8da0-53878819ce2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|         time_stamp|\n+-------+-------------------+\n|      3|2020-03-21 10:16:13|\n|      7|2020-01-04 13:57:59|\n|      2|2020-07-29 23:09:44|\n|      6|2020-12-09 10:39:37|\n+-------+-------------------+\n\n+-------+-------------------+---------+\n|user_id|         time_stamp|   action|\n+-------+-------------------+---------+\n|      3|2021-01-06 03:30:46|  timeout|\n|      3|2021-07-14 14:00:00|  timeout|\n|      7|2021-06-12 11:57:29|confirmed|\n|      7|2021-06-13 12:58:28|confirmed|\n|      7|2021-06-14 13:59:27|confirmed|\n|      2|2021-01-22 00:00:00|confirmed|\n|      2|2021-02-28 23:59:59|  timeout|\n+-------+-------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Signups table\n",
    "df14_signups_data = [\n",
    "    (3, \"2020-03-21 10:16:13\"),\n",
    "    (7, \"2020-01-04 13:57:59\"),\n",
    "    (2, \"2020-07-29 23:09:44\"),\n",
    "    (6, \"2020-12-09 10:39:37\")\n",
    "]\n",
    "df14_signups_schema = [\"user_id\", \"time_stamp\"]\n",
    "\n",
    "# Confirmations table\n",
    "df14_confirmations_data = [\n",
    "    (3, \"2021-01-06 03:30:46\", \"timeout\"),\n",
    "    (3, \"2021-07-14 14:00:00\", \"timeout\"),\n",
    "    (7, \"2021-06-12 11:57:29\", \"confirmed\"),\n",
    "    (7, \"2021-06-13 12:58:28\", \"confirmed\"),\n",
    "    (7, \"2021-06-14 13:59:27\", \"confirmed\"),\n",
    "    (2, \"2021-01-22 00:00:00\", \"confirmed\"),\n",
    "    (2, \"2021-02-28 23:59:59\", \"timeout\")\n",
    "]\n",
    "df14_confirmations_schema = [\"user_id\", \"time_stamp\", \"action\"]\n",
    "\n",
    "# The confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. \n",
    "# The confirmation rate of a user that did not request any confirmation messages is 0. \n",
    "# Round the confirmation rate to two decimal places.\n",
    "\n",
    "# Write a solution to find the confirmation rate of each user.\n",
    "df14_signup = spark.createDataFrame(df14_signups_data, df14_signups_schema)\n",
    "df14_confirm = spark.createDataFrame(df14_confirmations_data, df14_confirmations_schema)\n",
    "\n",
    "df14_signup.show()\n",
    "df14_confirm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc90ef16-807d-42a1-ab39-b831da8318ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n|user_id|confirmation_rate|\n+-------+-----------------+\n|      3|              0.0|\n|      7|              1.0|\n|      2|              0.5|\n|      6|              0.0|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 14: SPARK\n",
    "\n",
    "df14_signup.join(df14_confirm, df14_signup[\"user_id\"] == df14_confirm[\"user_id\"], \"left\")\\\n",
    "  .groupBy(df14_signup[\"user_id\"]).agg( round(sum(when( df14_confirm[\"action\"] == \"confirmed\", 1).otherwise (0)) / count(\"*\") ,2).alias(\"confirmation_rate\"))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbea35c3-1d1c-4695-9b6f-60814d725f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n|user_id|confirmation_rate|\n+-------+-----------------+\n|      3|              0.0|\n|      7|              1.0|\n|      2|              0.5|\n|      6|              0.0|\n+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 14: SQL\n",
    "\n",
    "df14_signup.createOrReplaceTempView(\"d14_signup_tbl\")\n",
    "df14_confirm.createOrReplaceTempView(\"d14_confirm_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select s.user_id, round(sum(case when c.action=\"confirmed\" then 1 else 0 end)/count(*),2) as confirmation_rate \n",
    "          from d14_signup_tbl s left join d14_confirm_tbl c on s.user_id=c.user_id\n",
    "          group by s.user_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997eae20-9b45-4c3e-aa6f-4a0e48a2cf61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 15: Not Boring Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fc995a-45df-4cb4-9bf4-5ad00e502815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n| id|     movie|description|rating|\n+---+----------+-----------+------+\n|  1|       War|   great 3D|   8.9|\n|  2|   Science|    fiction|   8.5|\n|  3|     irish|     boring|   6.2|\n|  4|  Ice song|    Fantacy|   8.6|\n|  5|House card|Interesting|   9.1|\n+---+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df15_cinema_data = [\n",
    "    (1, \"War\", \"great 3D\", 8.9),\n",
    "    (2, \"Science\", \"fiction\", 8.5),\n",
    "    (3, \"irish\", \"boring\", 6.2),\n",
    "    (4, \"Ice song\", \"Fantacy\", 8.6),\n",
    "    (5, \"House card\", \"Interesting\", 9.1)\n",
    "]\n",
    "df15_cinema_schema = [\"id\", \"movie\", \"description\", \"rating\"]\n",
    "\n",
    "df15 = spark.createDataFrame(df15_cinema_data, df15_cinema_schema)\n",
    "\n",
    "# Write a solution to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "# Return the result table ordered by rating in descending order.\n",
    "\n",
    "df15.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0ef20a-33f5-4398-b055-4f8eab135802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n| id|     movie|description|rating|\n+---+----------+-----------+------+\n|  5|House card|Interesting|   9.1|\n|  1|       War|   great 3D|   8.9|\n+---+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 15: SPARK\n",
    "\n",
    "df15.select(\"*\")\\\n",
    "    .filter((col(\"id\")%2==1) & (col(\"description\")!=\"boring\"))\\\n",
    "        .orderBy(desc(\"rating\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0341bb21-eadf-445a-a515-92bb7fbf3f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n| id|     movie|description|rating|\n+---+----------+-----------+------+\n|  5|House card|Interesting|   9.1|\n|  1|       War|   great 3D|   8.9|\n+---+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 15: SQL\n",
    "\n",
    "df15.createOrReplaceTempView(\"d15_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select * \n",
    "            from d15_tbl where (id%2)=1 and description <> \"boring\"\n",
    "            order by rating desc;\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df892e7d-54aa-45a9-97f4-56e42d9d57a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 16: Average Selling Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d24bff-e0c6-4ee5-a28c-1997fd2ea91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+\n|product_id|start_date|  end_date|price|\n+----------+----------+----------+-----+\n|         1|2019-02-17|2019-02-28|    5|\n|         1|2019-03-01|2019-03-22|   20|\n|         2|2019-02-01|2019-02-20|   15|\n|         2|2019-02-21|2019-03-31|   30|\n+----------+----------+----------+-----+\n\n+----------+-------------+-----+\n|product_id|purchase_date|units|\n+----------+-------------+-----+\n|         1|   2019-02-25|  100|\n|         1|   2019-03-01|   15|\n|         2|   2019-02-10|  200|\n|         2|   2019-03-22|   30|\n+----------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df16_prices_data = [\n",
    "    (1, \"2019-02-17\", \"2019-02-28\", 5),\n",
    "    (1, \"2019-03-01\", \"2019-03-22\", 20),\n",
    "    (2, \"2019-02-01\", \"2019-02-20\", 15),\n",
    "    (2, \"2019-02-21\", \"2019-03-31\", 30)\n",
    "]\n",
    "df16_prices_schema = [\"product_id\", \"start_date\", \"end_date\", \"price\"]\n",
    "\n",
    "df16_units_sold_data = [\n",
    "    (1, \"2019-02-25\", 100),\n",
    "    (1, \"2019-03-01\", 15),\n",
    "    (2, \"2019-02-10\", 200),\n",
    "    (2, \"2019-03-22\", 30)\n",
    "]\n",
    "df16_units_sold_schema = [\"product_id\", \"purchase_date\", \"units\"]\n",
    "\n",
    "# Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places. If a product does not have any sold units, its average selling price is assumed to be 0.\n",
    "\n",
    "df16_prices = spark.createDataFrame(df16_prices_data, df16_prices_schema)\n",
    "df16_units = spark.createDataFrame(df16_units_sold_data, df16_units_sold_schema)\n",
    "\n",
    "df16_prices.show()\n",
    "df16_units.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a25dc6-0eef-4659-9be6-95ac80395e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|product_id|average_price|\n+----------+-------------+\n|         1|         6.96|\n|         2|        16.96|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 16: SPARK\n",
    "\n",
    "df16_units.join(df16_prices, (df16_units[\"product_id\"] == df16_prices[\"product_id\"]) & (df16_units[\"purchase_date\"] >= df16_prices[\"start_date\"]) & (df16_units[\"purchase_date\"] <= df16_prices[\"end_date\"]))\\\n",
    "    .groupBy(df16_units[\"product_id\"]).agg(round((sum(df16_units[\"units\"] * df16_prices[\"price\"])) / (sum(df16_units[\"units\"])),2).alias(\"average_price\"))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d202569e-c520-4d2b-be7d-69783e9b0eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|product_id|average_price|\n+----------+-------------+\n|         1|         6.96|\n|         2|        16.96|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 16: SQL\n",
    "\n",
    "df16_prices.createOrReplaceTempView(\"d16_prices_tbl\")\n",
    "df16_units.createOrReplaceTempView(\"d16_units_tbl\")\n",
    "spark.sql('''\n",
    "          select u.product_id, round(sum(u.units*p.price)/(sum(u.units)),2) as average_price\n",
    "        from d16_prices_tbl p inner join d16_units_tbl u on p.product_id=u.product_id\n",
    "        and u.purchase_date>=p.start_date and u.purchase_date<= p.end_date\n",
    "        group by u.product_id;\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6089032d-d087-4449-bbcb-4567e0187171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 17: Project Employees I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8ac4c2-20af-4a54-97ef-4b5f67f7e866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|project_id|employee_id|\n+----------+-----------+\n|         1|          1|\n|         1|          2|\n|         1|          3|\n|         2|          1|\n|         2|          4|\n+----------+-----------+\n\n+-----------+------+----------------+\n|employee_id|  name|experience_years|\n+-----------+------+----------------+\n|          1|Khaled|               3|\n|          2|   Ali|               2|\n|          3|  John|               1|\n|          4|   Doe|               2|\n+-----------+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df17_project_data = [\n",
    "    (1, 1),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 1),\n",
    "    (2, 4)\n",
    "]\n",
    "df17_project_schema = [\"project_id\", \"employee_id\"]\n",
    "\n",
    "df17_employee_data = [\n",
    "    (1, \"Khaled\", 3),\n",
    "    (2, \"Ali\", 2),\n",
    "    (3, \"John\", 1),\n",
    "    (4, \"Doe\", 2)\n",
    "]\n",
    "df17_employee_schema = [\"employee_id\", \"name\", \"experience_years\"]\n",
    "\n",
    "df17_project = spark.createDataFrame(df17_project_data, df17_project_schema)\n",
    "df17_employee = spark.createDataFrame(df17_employee_data, df17_employee_schema)\n",
    "\n",
    "# Write an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.\n",
    "\n",
    "df17_project.show()\n",
    "df17_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35513532-0154-468e-94f9-7c585ccbb53a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|project_id|average_years|\n+----------+-------------+\n|         1|          2.0|\n|         2|          2.5|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 17: SPARK\n",
    "\n",
    "df17_project.join(df17_employee, df17_project[\"employee_id\"] == df17_employee[\"employee_id\"], \"inner\")\\\n",
    "    .groupBy(df17_project[\"project_id\"]).agg(round(avg(df17_employee[\"experience_years\"]),2).alias(\"average_years\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d71adf-5202-4c6c-9b20-ec15ae0faeac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|project_id|average_years|\n+----------+-------------+\n|         1|          2.0|\n|         2|          2.5|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 17: SQL\n",
    "df17_employee.createOrReplaceTempView(\"df17_emp_tbl\")\n",
    "df17_project.createOrReplaceTempView(\"df17_pro_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          SELECT p.project_id, ROUND(avg(e.experience_years), 2) as average_years\n",
    "            from df17_pro_tbl p inner join df17_emp_tbl e \n",
    "            on p.employee_id=e.employee_id\n",
    "            group by p.project_id;\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3227c6b-dd54-4e16-aa38-e532f946cb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 18: Percentage of Users Attended a Contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6133d525-5c92-4256-9ae4-ceeaecdaad4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n|user_id|user_name|\n+-------+---------+\n|      6|    Alice|\n|      2|      Bob|\n|      7|     Alex|\n+-------+---------+\n\n+----------+-------+\n|contest_id|user_id|\n+----------+-------+\n|       215|      6|\n|       209|      2|\n|       208|      2|\n|       210|      6|\n|       208|      6|\n|       209|      7|\n|       209|      6|\n|       215|      7|\n|       208|      7|\n|       210|      2|\n|       207|      2|\n|       210|      7|\n+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df18_users_data = [\n",
    "    (6, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (7, \"Alex\")\n",
    "]\n",
    "df18_users_schema = [\"user_id\", \"user_name\"]\n",
    "\n",
    "df18_register_data = [\n",
    "    (215, 6), (209, 2), (208, 2), (210, 6),\n",
    "    (208, 6), (209, 7), (209, 6), (215, 7),\n",
    "    (208, 7), (210, 2), (207, 2), (210, 7)\n",
    "]\n",
    "df18_register_schema = [\"contest_id\", \"user_id\"]\n",
    "\n",
    "df18_users = spark.createDataFrame(df18_users_data, df18_users_schema)\n",
    "df18_register = spark.createDataFrame(df18_register_data, df18_register_schema)\n",
    "\n",
    "df18_users.show()\n",
    "df18_register.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5394b1-3137-4641-b60f-f0d704dffd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|       3|\n+--------+\n\nNone\nDataFrame[user_id: bigint, user_name: string]\n3\n[Row(user_id=6, user_name='Alice'), Row(user_id=2, user_name='Bob'), Row(user_id=7, user_name='Alex')]\n[Row(user_id=7, user_name='Alex')]\n6\n3\n+----------+----------+\n|contest_id|percentage|\n+----------+----------+\n|       208|     100.0|\n|       209|     100.0|\n|       210|     100.0|\n|       215|     66.67|\n|       207|     33.33|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 18: SPARK\n",
    "\n",
    "# df18_register.groupBy(\"contest_id\").agg(count(\"*\") / df18_users.select(count(\"*\")) ).show()\n",
    "# You cannot place a full DataFrame operation (select(count(*))) inside an aggregation function directly like that.\n",
    "\n",
    "print(df18_users.select(count(\"*\")).show())\n",
    "\n",
    "# ACCESSING VALUES IN SPARK\n",
    "print(df18_users.select(\"*\"))\n",
    "print(df18_users.select(\"*\").count())\n",
    "\n",
    "print(df18_users.select(\"*\").collect())\n",
    "\n",
    "print(df18_users.select(\"*\").collect()[-1:])\n",
    "print(df18_users.select(\"*\").collect()[0][0])\n",
    "# So,\n",
    "total_users = df18_users.count()\n",
    "print(total_users)\n",
    "\n",
    "df18_register.groupBy(\"contest_id\").agg(round((count(\"*\") /total_users) * 100,2).alias(\"percentage\") )\\\n",
    "    .sort(col(\"percentage\").desc(), col(\"contest_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff2cf83e-7bf1-4f3c-9cba-2bf2d8c001e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|contest_id|percentage|\n+----------+----------+\n|       208|     100.0|\n|       209|     100.0|\n|       210|     100.0|\n|       215|     66.67|\n|       207|     33.33|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 18: SQL\n",
    "\n",
    "df18_users.createOrReplaceTempView(\"df18_users_tbl\")\n",
    "df18_register.createOrReplaceTempView(\"df18_register_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select r.contest_id, round(count(r.user_id)/ (select count(distinct user_id) from df18_users_tbl)*100, 2) as percentage\n",
    "            from df18_register_tbl r\n",
    "            group by r.contest_id\n",
    "            order by percentage desc, r.contest_id;\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1493ea85-155c-45bc-bcda-d2fbc4c72218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 19: Queries Quality and Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0ec010-39d0-4415-88a3-a515ec47e384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+------+\n|query_name|          result|position|rating|\n+----------+----------------+--------+------+\n|       Dog|Golden Retriever|       1|     5|\n|       Dog| German Shepherd|       2|     5|\n|       Dog|            Mule|     200|     1|\n|       Cat|         Shirazi|       5|     2|\n|       Cat|         Siamese|       3|     3|\n|       Cat|          Sphynx|       7|     4|\n+----------+----------------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df19_queries_data = [\n",
    "    (\"Dog\", \"Golden Retriever\", 1, 5),\n",
    "    (\"Dog\", \"German Shepherd\", 2, 5),\n",
    "    (\"Dog\", \"Mule\", 200, 1),\n",
    "    (\"Cat\", \"Shirazi\", 5, 2),\n",
    "    (\"Cat\", \"Siamese\", 3, 3),\n",
    "    (\"Cat\", \"Sphynx\", 7, 4)\n",
    "]\n",
    "df19_queries_schema = [\"query_name\", \"result\", \"position\", \"rating\"]\n",
    "\n",
    "# We define query quality as: The average of the ratio between query rating and its position.\n",
    "# We also define poor query percentage as: The percentage of all queries with rating less than 3.\n",
    "# Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "# Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "df19 = spark.createDataFrame(data=df19_queries_data, schema=df19_queries_schema)\n",
    "df19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d790d04-c818-4f10-802e-cf12c943a55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------------+\n|query_name|quality|poor_query_percentage|\n+----------+-------+---------------------+\n|       Dog|    2.5|                33.33|\n|       Cat|   0.66|                33.33|\n+----------+-------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 19: SPARK\n",
    "\n",
    "# df19.groupBy(col(\"query_name\")).agg(round(sum( (col(\"rating\"))/(col(\"position\")) ) / (count(\"*\")),2).alias(\"quality\"))    .agg((sum(when(col(\"rating\")<3, 1).otherwise(0))/(count(\"*\"))*100).alias(\"poor_query_percentage\")).show()\n",
    "\n",
    "#Chaining .agg() twice is INCORRECT. Instead use one .agg() with multiple metrics\n",
    "\n",
    "df19.groupBy(col(\"query_name\")).agg(round(avg( (col(\"rating\"))/(col(\"position\")) ),2).alias(\"quality\") , round(avg(when(col(\"rating\")<3, 1).otherwise(0))*100, 2).alias(\"poor_query_percentage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3d414e-faca-49d6-8545-5486e5a73a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------------------+\n|query_name|           quality|poor_query_percentage|\n+----------+------------------+---------------------+\n|       Dog|2.5016666666666665|                 0.33|\n|       Cat|0.6571428571428571|                 0.33|\n+----------+------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 19: SQL\n",
    "\n",
    "df19.createOrReplaceTempView(\"df19_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select query_name, avg(rating/position) as quality, round(avg(case when rating<3 then 1 else 0 end),2) as poor_query_percentage from df19_tbl\n",
    "          group by query_name\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b439c0-0ed5-4ce0-9b7a-468d33f6d80d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 20: Monthly Transactions I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811c3555-aadd-4fc2-84ff-f703a06e8599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+----------+\n| id|country|   state|amount|trans_date|\n+---+-------+--------+------+----------+\n|121|     US|approved|  1000|2018-12-18|\n|122|     US|declined|  2000|2018-12-19|\n|123|     US|approved|  2000|2019-01-01|\n|124|     DE|approved|  2000|2019-01-07|\n+---+-------+--------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df20_transaction_data = [\n",
    "    (121, \"US\", \"approved\", 1000, \"2018-12-18\"),\n",
    "    (122, \"US\", \"declined\", 2000, \"2018-12-19\"),\n",
    "    (123, \"US\", \"approved\", 2000, \"2019-01-01\"),\n",
    "    (124, \"DE\", \"approved\", 2000, \"2019-01-07\"),\n",
    "]\n",
    "\n",
    "df20_transaction_schema = [\"id\", \"country\", \"state\", \"amount\", \"trans_date\"]\n",
    "\n",
    "# Write an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.\n",
    "\n",
    "df20 = spark.createDataFrame(data=df20_transaction_data, schema=df20_transaction_schema)\n",
    "df20.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df79760-9b44-4fa0-b064-829c7da355da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----------+--------------+------------------+---------------------+\n|trans_date_dm|country|trans_count|approved_count|trans_total_amount|approved_total_amount|\n+-------------+-------+-----------+--------------+------------------+---------------------+\n|      2018-12|     US|          2|             1|              3000|                 1000|\n|      2019-01|     US|          1|             1|              2000|                 2000|\n|      2019-01|     DE|          1|             1|              2000|                 2000|\n+-------------+-------+-----------+--------------+------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 20: SPARK\n",
    "\n",
    "# to_date -> converts a string column to date format\n",
    "# date_format -> does formatting on date column\n",
    "\n",
    "df20.withColumn(\"trans_date_dm\", date_format(col(\"trans_date\"), \"yyyy-MM\"))\\\n",
    "    .groupBy(col(\"trans_date_dm\"), col(\"country\")).agg(count(\"*\").alias(\"trans_count\"), sum(when(col(\"state\")==\"approved\", 1).otherwise(0)).alias(\"approved_count\"), sum(col(\"amount\")).alias(\"trans_total_amount\"), sum(when(col(\"state\") == \"approved\", col(\"amount\")).otherwise(0)).alias(\"approved_total_amount\")   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ff2265-3259-49ff-8e23-e756c99cd4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+--------------+------------------+---------------------+\n|trans_date_m|country|trans_count|approved_count|trans_total_amount|approved_total_amount|\n+------------+-------+-----------+--------------+------------------+---------------------+\n|     2018-12|     US|          2|             1|              3000|                 1000|\n|     2019-01|     US|          1|             1|              2000|                 2000|\n|     2019-01|     DE|          1|             1|              2000|                 2000|\n+------------+-------+-----------+--------------+------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 20: SQL\n",
    "df20.createOrReplaceTempView(\"d20_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select date_format(trans_date, \"yyyy-MM\") as trans_date_m, country, count(*) as trans_count, sum(case when state=\"approved\" then 1 else 0 end) as approved_count,\n",
    "          sum(amount) as trans_total_amount, sum(case when state=\"approved\" then amount else 0 end) as approved_total_amount \n",
    "          from d20_tbl\n",
    "          group by trans_date_m, country\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4b1f1b-12ec-420b-9763-6f4c9086db04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 21: Immediate Food Delivery II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee2b5e4-2452-4d53-91ff-b6d0b6caaaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SELF JOIN RULES**\n",
    "\n",
    "In a join between two DataFrames with overlapping column names (like customer_id or order_date), Spark needs you to disambiguate where the column comes from.\n",
    "Otherwise, it throws an AnalysisException: Column ... are ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efbeb9f-5ca3-49fa-b960-7c1a5070e25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|customer_id|order_date|\n+-----------+----------+\n|          1|2023-01-01|\n|          2|2023-01-02|\n|          1|2023-01-03|\n+-----------+----------+\n\n+-----------+----------+-----------+\n|customer_id|left_order|right_order|\n+-----------+----------+-----------+\n|          1|2023-01-01| 2023-01-01|\n|          1|2023-01-01| 2023-01-03|\n|          1|2023-01-03| 2023-01-01|\n|          1|2023-01-03| 2023-01-03|\n|          2|2023-01-02| 2023-01-02|\n+-----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Join a DataFrame to itself (or to another with overlapping column names), and select columns.\n",
    "\n",
    "data = [\n",
    "    (1, \"2023-01-01\"),\n",
    "    (2, \"2023-01-02\"),\n",
    "    (1, \"2023-01-03\")\n",
    "]\n",
    "schema = [\"customer_id\", \"order_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# DO NO DO THE BELOW: \n",
    "# df.join(df, df[\"customer_id\"] == df[\"customer_id\"]).select(\"customer_id\", \"order_date\").show()\n",
    "\n",
    "# Correct Way: Use .alias() to disambiguate\n",
    "df1 = df.alias(\"a\")\n",
    "df2 = df.alias(\"b\")\n",
    "\n",
    "df1.join(df2, col(\"a.customer_id\") == col(\"b.customer_id\")) \\\n",
    "   .select(col(\"a.customer_id\"), col(\"a.order_date\").alias(\"left_order\"), col(\"b.order_date\").alias(\"right_order\")) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b3e846-743d-41dd-bced-af896464b5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------------------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|\n+-----------+-----------+----------+---------------------------+\n|          1|          1|2019-08-01|                 2019-08-02|\n|          2|          2|2019-08-02|                 2019-08-02|\n|          3|          1|2019-08-11|                 2019-08-12|\n|          4|          3|2019-08-24|                 2019-08-24|\n|          5|          3|2019-08-21|                 2019-08-22|\n|          6|          2|2019-08-11|                 2019-08-13|\n|          7|          4|2019-08-09|                 2019-08-09|\n+-----------+-----------+----------+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "d21_delivery_data = [\n",
    "    (1, 1, \"2019-08-01\", \"2019-08-02\"),\n",
    "    (2, 2, \"2019-08-02\", \"2019-08-02\"),\n",
    "    (3, 1, \"2019-08-11\", \"2019-08-12\"),\n",
    "    (4, 3, \"2019-08-24\", \"2019-08-24\"),\n",
    "    (5, 3, \"2019-08-21\", \"2019-08-22\"),\n",
    "    (6, 2, \"2019-08-11\", \"2019-08-13\"),\n",
    "    (7, 4, \"2019-08-09\", \"2019-08-09\"),\n",
    "]\n",
    "\n",
    "d21_delivery_schema = [\"delivery_id\", \"customer_id\", \"order_date\", \"customer_pref_delivery_date\"]\n",
    "\n",
    "# If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "# The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "# Write a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.\n",
    "\n",
    "df21 = spark.createDataFrame(data=d21_delivery_data, schema=d21_delivery_schema)\n",
    "\n",
    "df21.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c3e69c-57bb-4ea9-a414-2ff155930ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|immediate_percentage|\n+--------------------+\n|                 0.5|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 21: SPARK\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(col(\"customer_id\")).orderBy(col(\"order_date\"))\n",
    "\n",
    "df21.withColumn(\"rank_delivery\", rank().over(window))\\\n",
    "    .filter(col(\"rank_delivery\") == 1)\\\n",
    "        .withColumn(\"type\", when(col(\"order_date\") == col(\"customer_pref_delivery_date\"), \"immediate\").otherwise(\"scheduled\"))\\\n",
    "            .groupBy().agg((sum(when(col(\"type\")==\"immediate\", 1).otherwise(0))/count(\"*\")).alias(\"immediate_percentage\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccdb6bd-8738-4b67-926d-907a8dc2266a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n+-----------+-----------+\n|customer_id|first_order|\n+-----------+-----------+\n|          1| 2019-08-01|\n|          2| 2019-08-02|\n|          3| 2019-08-21|\n|          4| 2019-08-09|\n+-----------+-----------+\n\n+--------------------+\n|immediate_percentage|\n+--------------------+\n|                 0.5|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# OPTION 2\n",
    "\n",
    "a = df21.groupBy(col(\"customer_id\")).agg(min(col(\"order_date\")).alias(\"first_order\"))\n",
    "print(type(a))\n",
    "a.show()\n",
    "a.join(df21.alias(\"b\"), (col(\"b.customer_id\") == a[\"customer_id\"]) & (col(\"b.order_date\") == a[\"first_order\"]), \"inner\")\\\n",
    "    .select(round(sum(when(col(\"b.order_date\") == col(\"b.customer_pref_delivery_date\"), 1).otherwise(0))/ count(\"*\"),2).alias(\"immediate_percentage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d182cb2-dbc8-4987-99d0-09259c85552b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|immediate_percentage|\n+--------------------+\n|                 0.5|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 21: SQL\n",
    "\n",
    "df21.createOrReplaceTempView(\"df21_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select round(avg(case when order_date= customer_pref_delivery_date then 1 else 0 end),2) as immediate_percentage\n",
    "          from df21_tbl where (customer_id, order_date) IN (\n",
    "          select customer_id, min(order_date) as first_order from df21_tbl group by customer_id)\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af3d330-ff58-4a13-bc58-54c3f5354e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|customer_id|order_date|\n+-----------+----------+\n|          1|2023-01-01|\n|          2|2023-01-02|\n|          1|2023-01-03|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"2023-01-01\"),\n",
    "    (2, \"2023-01-02\"),\n",
    "    (1, \"2023-01-03\")\n",
    "]\n",
    "schema = [\"customer_id\", \"order_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653d2afd-7df1-4d8f-ad97-08a37a33adf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 22: Game Play Analysis IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7e8e0f-4bae-4bad-b482-0b19066c1e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-03-02|           6|\n|        2|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n+---------+---------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "d22_activity_data = [\n",
    "    (1, 2, \"2016-03-01\", 5),\n",
    "    (1, 2, \"2016-03-02\", 6),\n",
    "    (2, 3, \"2017-06-25\", 1),\n",
    "    (3, 1, \"2016-03-02\", 0),\n",
    "    (3, 4, \"2018-07-03\", 5)\n",
    "]\n",
    "\n",
    "d22_activity_schema = [\"player_id\", \"device_id\", \"event_date\", \"games_played\"]\n",
    "\n",
    "# Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. In other words, you need to determine the number of players who logged in on the day immediately following their initial login, and divide it by the number of total players.\n",
    "\n",
    "df22 = spark.createDataFrame(data= d22_activity_data, schema=d22_activity_schema)\n",
    "\n",
    "df22.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8599710-16dc-475d-903d-d371aabf2caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|          fraction|\n+------------------+\n|0.3333333333333333|\n+------------------+\n\n3\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 22: SPARK\n",
    "\n",
    "df22_min = df22.groupBy(col(\"player_id\")).agg(min(col(\"event_date\")).alias(\"first_date\")).alias(\"a\")\n",
    "\n",
    "df22.join(df22_min, (col(\"a.player_id\") == df22[\"player_id\"]) & (datediff(df22[\"event_date\"], col(\"a.first_date\")) ==1))\\\n",
    "    .select((count(\"*\") / (df22.select(countDistinct(col(\"player_id\"))).collect()[0][0])).alias(\"fraction\")).show()\n",
    "\n",
    "print(df22.select(countDistinct(col(\"player_id\"))).collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beab3f95-7215-4e0b-8921-ccf46ebf63c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- print(df22.count()) is used to count lines in entire **dataframe**. \n",
    "- Count non-null values in a specific column:  df22.select(count(\"player_id\")).show()\n",
    "- Count distinct values in a column:  df22.select(countDistinct(\"player_id\")).show()\n",
    "\n",
    "All the above will give output as dataframes.\n",
    "- To get value:  df22.select(count(\"player_id\")).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654e7c0d-1cd4-483c-83b3-957b91fa06af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|fraction|\n+--------+\n|    0.33|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 22: SQL\n",
    "df22.createOrReplaceTempView(\"df22_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select round(count(*)/(SELECT count(DISTINCT player_id) from df22_tbl),2) as fraction from df22_tbl a inner join\n",
    "          (select player_id, min(event_date) as first_game from df22_tbl group by player_id) b \n",
    "          on a.player_id = b.player_id and date_diff(a.event_date, b.first_game)=1\n",
    "          ''').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a733865-9c65-470c-9a9c-09effd40b099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 23: Number of Unique Subjects Taught by Each Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2bba10-32c8-4fa2-816c-d79bfde71a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n|teacher_id|subject_id|dept_id|\n+----------+----------+-------+\n|         1|         2|      3|\n|         1|         2|      4|\n|         1|         3|      3|\n|         2|         1|      1|\n|         2|         2|      1|\n|         2|         3|      1|\n|         2|         4|      1|\n+----------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df23_teacher_data = [\n",
    "    (1, 2, 3),\n",
    "    (1, 2, 4),\n",
    "    (1, 3, 3),\n",
    "    (2, 1, 1),\n",
    "    (2, 2, 1),\n",
    "    (2, 3, 1),\n",
    "    (2, 4, 1)\n",
    "]\n",
    "\n",
    "df23_teacher_schema = [\"teacher_id\", \"subject_id\", \"dept_id\"]\n",
    "\n",
    "# Write a solution to calculate the number of unique subjects each teacher teaches in the university.\n",
    "\n",
    "df23 = spark.createDataFrame(df23_teacher_data, schema=df23_teacher_schema)\n",
    "df23.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e72c3e-9ec7-4e3f-b924-cc3b341b9ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n|teacher_id|cnt|\n+----------+---+\n|         1|  2|\n|         2|  4|\n+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 23: SPARK\n",
    "\n",
    "df23.groupBy(col(\"teacher_id\")).agg(countDistinct(col(\"subject_id\")).alias(\"cnt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9d34bb-2ce9-43e5-94da-1010becae61f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n|teacher_id|cnt|\n+----------+---+\n|         1|  2|\n|         2|  4|\n+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 23: SQL\n",
    "\n",
    "df23.createOrReplaceTempView(\"df23_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select teacher_id, count(distinct subject_id) as cnt from df23_tbl\n",
    "          group by teacher_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9be4a5-e98e-472a-a2a3-ea7405bba22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 24: User Activity for the Past 30 Days I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767e0be4-88ba-46d3-b8f6-4e258bf2a8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+-------------+\n|user_id|session_id|activity_date|activity_type|\n+-------+----------+-------------+-------------+\n|      1|         1|   2019-07-20| open_session|\n|      1|         1|   2019-07-20|  scroll_down|\n|      1|         1|   2019-07-20|  end_session|\n|      2|         4|   2019-07-20| open_session|\n|      2|         4|   2019-07-21| send_message|\n|      2|         4|   2019-07-21|  end_session|\n|      3|         2|   2019-07-21| open_session|\n|      3|         2|   2019-07-21| send_message|\n|      3|         2|   2019-07-21|  end_session|\n|      4|         3|   2019-06-25| open_session|\n|      4|         3|   2019-06-25|  end_session|\n+-------+----------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df24_activity_data = [\n",
    "    (1, 1, \"2019-07-20\", \"open_session\"),\n",
    "    (1, 1, \"2019-07-20\", \"scroll_down\"),\n",
    "    (1, 1, \"2019-07-20\", \"end_session\"),\n",
    "    (2, 4, \"2019-07-20\", \"open_session\"),\n",
    "    (2, 4, \"2019-07-21\", \"send_message\"),\n",
    "    (2, 4, \"2019-07-21\", \"end_session\"),\n",
    "    (3, 2, \"2019-07-21\", \"open_session\"),\n",
    "    (3, 2, \"2019-07-21\", \"send_message\"),\n",
    "    (3, 2, \"2019-07-21\", \"end_session\"),\n",
    "    (4, 3, \"2019-06-25\", \"open_session\"),\n",
    "    (4, 3, \"2019-06-25\", \"end_session\")\n",
    "]\n",
    "\n",
    "df24_activity_schema = [\"user_id\", \"session_id\", \"activity_date\", \"activity_type\"]\n",
    "\n",
    "# Write a solution to find the daily active user count for a period of 30 days ending 2019-07-27 inclusively. A user was active on someday if they made at least one activity on that day.\n",
    "\n",
    "df24 = spark.createDataFrame(df24_activity_data, df24_activity_schema)\n",
    "df24.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6c7732-9922-48a7-b636-1962c31efd1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n|activity_date|active_users|\n+-------------+------------+\n|   2019-07-20|           2|\n|   2019-07-21|           2|\n+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 24: SPARK\n",
    "\n",
    "df24.groupBy(\"activity_date\").agg(countDistinct(col(\"user_id\")).alias(\"active_users\"))\\\n",
    "    .sort(col(\"activity_date\"))\\\n",
    "        .filter((col(\"activity_date\")>= date_sub(lit('2019-07-27'),30))  &  (col(\"activity_date\")<= '2019-07-27') ).show()\n",
    "\n",
    "# lit('2019-07-27') — turns the date string into a column-like literal\n",
    "# NOTE: Here date_sub(<column_name>, <int>)\n",
    "# OR\n",
    "# date_sub(lit('<date_string>'), <days:int>)\n",
    "# Unlike SQL where <date: int> = INTERVAL 30 DAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b2eb1b-3ac0-4ea9-b1a8-25a029d843d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n|activity_date|active_users|\n+-------------+------------+\n|   2019-07-21|           2|\n|   2019-07-20|           2|\n+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 24: SQL\n",
    "\n",
    "df24.createOrReplaceTempView(\"df24_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select activity_date, count(distinct user_id) as active_users from df24_tbl\n",
    "          where activity_date<='2019-07-27' and activity_date>= date_sub('2019-07-27', 30)\n",
    "          group by activity_date\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0557e617-5a78-481f-947b-2bcc05624591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 25: Product Sales Analysis III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73eeeba2-144d-4690-96a3-d425a6614519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n|sale_id|product_id|year|quantity|price|\n+-------+----------+----+--------+-----+\n|      1|       100|2008|      10| 5000|\n|      2|       100|2009|      12| 5000|\n|      7|       200|2011|      15| 9000|\n+-------+----------+----+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df25_sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "df25_sales_schema = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
    "\n",
    "# Write a solution to find all sales that occurred in the first year each product was sold.\n",
    "# For each product_id, identify the earliest year it appears in the Sales table.\n",
    "# Return all sales entries for that product in that year.\n",
    "# Return a table with the following columns: product_id, first_year, quantity, and price.\n",
    "\n",
    "df25 = spark.createDataFrame(df25_sales_data, df25_sales_schema)\n",
    "\n",
    "# Write a solution to find all sales that occurred in the first year each product was sold.\n",
    "# For each product_id, identify the earliest year it appears in the Sales table.\n",
    "# Return all sales entries for that product in that year.\n",
    "# Return a table with the following columns: product_id, first_year, quantity, and price.\n",
    "\n",
    "df25.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da2c12e0-0245-4a0a-ac81-6f290bf20768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n|product_id|first_year|quantity|price|\n+----------+----------+--------+-----+\n|       100|      2008|      10| 5000|\n|       200|      2011|      15| 9000|\n+----------+----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 25: SPARK\n",
    "df25_min_year = df25.groupBy(col(\"product_id\")).agg(min(col(\"year\")).alias(\"first_year\")).alias(\"a\")\n",
    "\n",
    "df25.join(df25_min_year, df25[\"year\"] == col(\"a.first_year\"))\\\n",
    "    .select(col(\"a.product_id\"), col(\"a.first_year\"), df25[\"quantity\"], df25[\"price\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f257239d-8e8e-45cd-af00-48a4610f517e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n|product_id|first_year|quantity|price|\n+----------+----------+--------+-----+\n|       100|      2008|      10| 5000|\n|       200|      2011|      15| 9000|\n+----------+----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 25: SQL\n",
    "\n",
    "df25.createOrReplaceTempView(\"df25_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select product_id, year as first_year, quantity, price from df25_tbl where (product_id, year) IN\n",
    "          (select product_id, min(year) from df25_tbl \n",
    "          group by product_id)\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc71909b-4d5b-48ca-85b3-20a19acbbdda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 26: Classes With at Least 5 Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c83799-8946-4d3d-98b7-86906f3f0126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|student|   class|\n+-------+--------+\n|      A|    Math|\n|      B| English|\n|      C|    Math|\n|      D| Biology|\n|      E|    Math|\n|      F|Computer|\n|      G|    Math|\n|      H|    Math|\n|      I|    Math|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df26_courses_data = [\n",
    "    (\"A\", \"Math\"),\n",
    "    (\"B\", \"English\"),\n",
    "    (\"C\", \"Math\"),\n",
    "    (\"D\", \"Biology\"),\n",
    "    (\"E\", \"Math\"),\n",
    "    (\"F\", \"Computer\"),\n",
    "    (\"G\", \"Math\"),\n",
    "    (\"H\", \"Math\"),\n",
    "    (\"I\", \"Math\")\n",
    "]\n",
    "\n",
    "df26_courses_schema = [\"student\", \"class\"]\n",
    "\n",
    "# Write a solution to find all the classes that have at least five students.\n",
    "\n",
    "df26 = spark.createDataFrame(df26_courses_data, df26_courses_schema)\n",
    "df26.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd273b3-7ce2-428d-a890-5ec923c02d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|class|\n+-----+\n| Math|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 26: SPARK\n",
    "\n",
    "df26.groupBy(col(\"class\")).agg(count(col(\"student\")).alias(\"student_count\"))\\\n",
    "    .filter(col(\"student_count\")>=5)\\\n",
    "        .select(col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1347bc-6857-432a-8564-dea86cf919e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|class|\n+-----+\n| Math|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 26: SQL\n",
    "df26.createOrReplaceTempView(\"df26_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select class from df26_tbl\n",
    "          group by class\n",
    "          having count(student)>=5\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a13edc0-f02f-480f-ac22-dbd658c670b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 27: Find Followers Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cdac3d-0d1f-4b61-a690-be9c5d6e87d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n|user_id|follower_id|\n+-------+-----------+\n|      0|          1|\n|      1|          0|\n|      2|          0|\n|      2|          1|\n+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df27_followers_data = [\n",
    "    (0, 1),\n",
    "    (1, 0),\n",
    "    (2, 0),\n",
    "    (2, 1)\n",
    "]\n",
    "\n",
    "df27_followers_schema = [\"user_id\", \"follower_id\"]\n",
    "\n",
    "# Write a solution that will, for each user, return the number of followers.\n",
    "# Return the result table ordered by user_id in ascending order.\n",
    "\n",
    "df27 = spark.createDataFrame(df27_followers_data, df27_followers_schema)\n",
    "df27.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bdcb3a1-1865-42bd-ae40-6b1fa57eadbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n|user_id|followers_count|\n+-------+---------------+\n|      0|              1|\n|      1|              1|\n|      2|              2|\n+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 27: SPARK\n",
    "\n",
    "df27.groupBy(col(\"user_id\")).agg(count(col(\"follower_id\")).alias(\"followers_count\"))\\\n",
    "    .sort(col(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f24d912-285d-49ac-a403-791d3573aa5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n|user_id|followers_count|\n+-------+---------------+\n|      0|              1|\n|      1|              1|\n|      2|              2|\n+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 27: SQL\n",
    "\n",
    "df27.createOrReplaceTempView(\"df27_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select user_id, count(follower_id) as followers_count from df27_tbl\n",
    "          group by user_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2376c4e-1607-4e79-8988-b0532fbbf5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 28: Biggest Single Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc368081-a91d-4a04-ac44-701f2d6012d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  8|\n|  8|\n|  3|\n|  3|\n|  1|\n|  4|\n|  5|\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df28_1_mynumbers_data = [\n",
    "    (8,),\n",
    "    (8,),\n",
    "    (3,),\n",
    "    (3,),\n",
    "    (1,),\n",
    "    (4,),\n",
    "    (5,),\n",
    "    (6,)\n",
    "]\n",
    "\n",
    "df28_1_mynumbers_schema = [\"num\"]\n",
    "\n",
    "# A single number is a number that appeared only once in the MyNumbers table.\n",
    "# Find the largest single number. If there is no single number, report null.\n",
    "\n",
    "df28_1 = spark.createDataFrame(df28_1_mynumbers_data, df28_1_mynumbers_schema)\n",
    "df28_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19a0643-219b-4404-b9da-47c19aedfb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 28: SPARK\n",
    "\n",
    "# QUESTION 28: SPARK\n",
    "df28_1.groupBy(col(\"num\")).agg(count(col(\"num\")).alias(\"freq\"))\\\n",
    "    .sort(col(\"num\").desc())\\\n",
    "        .filter(col(\"freq\") == 1)\\\n",
    "            .select(first(col(\"num\")).alias(\"num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8aaf49-07e2-4116-aff2-21a36153e581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|max(num)|\n+--------+\n|       6|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 28: SQL\n",
    "\n",
    "df28_1.createOrReplaceTempView(\"df28_1_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select(max(num)) from(\n",
    "          select num from df28_1_tbl\n",
    "          group by num\n",
    "          having count(num)=1\n",
    "          order by num desc\n",
    "          LIMIT 1)\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4935b31-d3da-438e-b5f3-403d591efa7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|num|\n+---+\n|  8|\n|  8|\n|  7|\n|  7|\n|  3|\n|  3|\n|  3|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df28_2_mynumbers_data = [\n",
    "    (8,),\n",
    "    (8,),\n",
    "    (7,),\n",
    "    (7,),\n",
    "    (3,),\n",
    "    (3,),\n",
    "    (3,)\n",
    "]\n",
    "\n",
    "df28_2_mynumbers_schema = [\"num\"]\n",
    "\n",
    "# A single number is a number that appeared only once in the MyNumbers table.\n",
    "# Find the largest single number. If there is no single number, report null.\n",
    "\n",
    "df28_2 = spark.createDataFrame(df28_2_mynumbers_data, df28_2_mynumbers_schema)\n",
    "df28_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0bf0f0d-9423-46ef-80b8-7a790d582c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n| num|\n+----+\n|null|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 28: SPARK\n",
    "df28_2.groupBy(col(\"num\")).agg(count(col(\"num\")).alias(\"freq\"))\\\n",
    "    .sort(col(\"num\").desc())\\\n",
    "        .filter(col(\"freq\") == 1)\\\n",
    "            .select(first(col(\"num\")).alias(\"num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed9ed7f-729e-4fe3-8d43-49d12e829330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|max(num)|\n+--------+\n|    null|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 28: SQL\n",
    "\n",
    "df28_2.createOrReplaceTempView(\"df28_2_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select(max(num)) from(\n",
    "          select num from df28_2_tbl\n",
    "          group by num\n",
    "          having count(num)=1\n",
    "          order by num desc\n",
    "          LIMIT 1)\n",
    "          ''').show()\n",
    "\n",
    "# To get \"null\" use functions like max, min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e35c42b5-2613-4165-8888-64dd5d1c8cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 29: Customers Who Bought All Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a396f774-e549-40ab-90da-b4752c483637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|product_key|\n+-----------+\n|          5|\n|          6|\n+-----------+\n\n+-----------+-----------+\n|customer_id|product_key|\n+-----------+-----------+\n|          1|          5|\n|          2|          6|\n|          3|          5|\n|          3|          6|\n|          1|          6|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df29_customer_data = [\n",
    "    (1, 5),\n",
    "    (2, 6),\n",
    "    (3, 5),\n",
    "    (3, 6),\n",
    "    (1, 6)\n",
    "]\n",
    "\n",
    "df29_customer_schema = [\"customer_id\", \"product_key\"]\n",
    "\n",
    "df29_product_data = [\n",
    "    (5,),\n",
    "    (6,)\n",
    "]\n",
    "\n",
    "df29_product_schema = [\"product_key\"]\n",
    "\n",
    "# Write a solution to report the customer ids from the Customer table that bought all the products in the Product table.\n",
    "\n",
    "df29_product = spark.createDataFrame(df29_product_data, df29_product_schema)\n",
    "df29_customer = spark.createDataFrame(df29_customer_data, df29_customer_schema)\n",
    "\n",
    "df29_product.show()\n",
    "df29_customer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03aa548-4070-4579-be91-bacc666b3692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          3|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 29: SPARK\n",
    "\n",
    "# distinct and count on \"DATAFRAME\"\n",
    "total_products = df29_product.distinct().count()\n",
    "print(total_products)\n",
    "\n",
    "df29_customer.groupBy(col(\"customer_id\")).agg(countDistinct(col(\"product_key\")).alias(\"cust_pro_count\") )\\\n",
    "    .filter(col(\"cust_pro_count\") == total_products)\\\n",
    "        .select(col(\"customer_id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006d501f-8972-48e7-90d1-b04d825e8992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          3|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 29: SQL\n",
    "\n",
    "df29_product.createOrReplaceTempView(\"df29_product_tbl\")\n",
    "df29_customer.createOrReplaceTempView(\"df29_customer_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select customer_id from df29_customer_tbl\n",
    "          group by customer_id having count(distinct(product_key)) = (select count(distinct product_key) from df29_product_tbl)\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d20aed3-7b37-41ec-85bc-b16c8434c1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 30: The Number of Employees Which Report to Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0f2cce-d7ce-4673-8d7d-b5bb4c8cb850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+---+\n|employee_id|   name|reports_to|age|\n+-----------+-------+----------+---+\n|          9|  Hercy|      null| 43|\n|          6|  Alice|         9| 41|\n|          4|    Bob|         9| 36|\n|          2|Winston|      null| 37|\n+-----------+-------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df30_data = [\n",
    "    (9, \"Hercy\", None, 43),\n",
    "    (6, \"Alice\", 9, 41),\n",
    "    (4, \"Bob\", 9, 36),\n",
    "    (2, \"Winston\", None, 37)\n",
    "]\n",
    "\n",
    "df30_schema = [\"employee_id\", \"name\", \"reports_to\", \"age\"]\n",
    "\n",
    "df30 = spark.createDataFrame(df30_data, df30_schema)\n",
    "\n",
    "# For this problem, we will consider a manager an employee who has at least 1 other employee reporting to them.\n",
    "# Write a solution to report the ids and the names of all managers, the number of employees who report directly to them, and the average age of the reports rounded to the nearest integer.\n",
    "# Return the result table ordered by employee_id.\n",
    "\n",
    "df30.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2f5559-5c8e-4cc2-bf52-551b100da4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+\n|reports_to|reports_count|average_age|\n+----------+-------------+-----------+\n|      null|            0|       40.0|\n|         9|            2|       39.0|\n+----------+-------------+-----------+\n\n+-----------+-----+-------------+-----------+\n|employee_id| name|reports_count|average_age|\n+-----------+-----+-------------+-----------+\n|          9|Hercy|            2|       39.0|\n+-----------+-----+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 30: SPARK\n",
    "\n",
    "df30_m = df30.alias(\"m\")\n",
    "df30_e = df30.alias(\"e\")\n",
    "\n",
    "employee = df30_e.groupBy(col(\"reports_to\")).agg(count(col(\"reports_to\")).alias(\"reports_count\"), round(avg(col(\"age\")),0).alias(\"average_age\"))\n",
    "employee.show()\n",
    "\n",
    "employee.alias(\"e\").join(df30_m, col(\"e.reports_to\") == col(\"m.employee_id\"))\\\n",
    "    .select(col(\"m.employee_id\"), col(\"m.name\"), col(\"e.reports_count\"), col(\"e.average_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7172d2df-bec5-4255-a358-077dc111dbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+-------------+-----------+\n|employee_id| name|reports_count|reports_count|average_age|\n+-----------+-----+-------------+-------------+-----------+\n|          9|Hercy|            2|            2|       39.0|\n+-----------+-----+-------------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 30: SQL\n",
    "\n",
    "df30.createOrReplaceTempView(\"df30_tbl\")\n",
    "spark.sql('''\n",
    "          select a.employee_id, a.name, b.reports_count, b.reports_count, b.average_age from df30_tbl as a inner join(\n",
    "          select reports_to, count(reports_to) as reports_count, round(avg(age),0) as average_age from df30_tbl group by reports_to) b\n",
    "          on a.employee_id=b.reports_to\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c358a3ba-c283-4374-9968-bd8c4f9eb370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+---+\n|employee_id|   name|reports_to|age|\n+-----------+-------+----------+---+\n|          1|Michael|      null| 45|\n|          2|  Alice|         1| 38|\n|          3|    Bob|         1| 42|\n|          4|Charlie|         2| 34|\n|          5|  David|         2| 40|\n|          6|    Eve|         3| 37|\n|          7|  Frank|      null| 50|\n|          8|  Grace|      null| 48|\n+-----------+-------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data for DataFrame 30.2\n",
    "df30_2_data = [\n",
    "    (1, \"Michael\", None, 45),\n",
    "    (2, \"Alice\", 1, 38),\n",
    "    (3, \"Bob\", 1, 42),\n",
    "    (4, \"Charlie\", 2, 34),\n",
    "    (5, \"David\", 2, 40),\n",
    "    (6, \"Eve\", 3, 37),\n",
    "    (7, \"Frank\", None, 50),\n",
    "    (8, \"Grace\", None, 48)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "df30_2_schema = [\"employee_id\", \"name\", \"reports_to\", \"age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df30_2 = spark.createDataFrame(df30_2_data, df30_2_schema)\n",
    "\n",
    "df30_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c4da57-b29f-48f5-b658-2b0a3db78265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+\n|reports_to|reports_count|average_age|\n+----------+-------------+-----------+\n|      null|            0|       48.0|\n|         1|            2|       40.0|\n|         2|            2|       37.0|\n|         3|            1|       37.0|\n+----------+-------------+-----------+\n\n+-----------+-------+-------------+-----------+\n|employee_id|   name|reports_count|average_age|\n+-----------+-------+-------------+-----------+\n|          1|Michael|            2|       40.0|\n|          2|  Alice|            2|       37.0|\n|          3|    Bob|            1|       37.0|\n+-----------+-------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 30: SPARK\n",
    "\n",
    "df30_manager = df30_2.alias(\"m\")\n",
    "df30_emp = df30_2.alias(\"e\")\n",
    "\n",
    "# manager = df30_manager.select(col(\"m.employee_id\").alias(\"manager\"))\\\n",
    "    # .filter(col(\"reports_to\").isNull()).show()\n",
    "\n",
    "employee = df30_emp.groupBy(col(\"reports_to\")).agg(count(col(\"reports_to\")).alias(\"reports_count\"), round(avg(col(\"age\")),0).alias(\"average_age\"))\n",
    "employee.show()\n",
    "\n",
    "employee.alias(\"e\").join(df30_manager, col(\"e.reports_to\") == col(\"m.employee_id\"))\\\n",
    "    .select(col(\"m.employee_id\"), col(\"m.name\"), col(\"e.reports_count\"), col(\"e.average_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bd50ad-3600-41ac-917e-06833d114bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----------+\n|employee_id|   name|reports_count|average_age|\n+-----------+-------+-------------+-----------+\n|          1|Michael|            2|       40.0|\n|          2|  Alice|            2|       37.0|\n|          3|    Bob|            1|       37.0|\n+-----------+-------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 30: SPARK\n",
    "\n",
    "df30_manager = df30_2.alias(\"mm\")\n",
    "df30_emp = df30_2.alias(\"ee\")\n",
    "\n",
    "df30_manager.join(df30_emp, col(\"mm.employee_id\") == col(\"ee.reports_to\"), \"inner\")\\\n",
    "    .groupBy(col(\"mm.employee_id\"), col(\"mm.name\")).agg(count(\"*\").alias(\"reports_count\"), round(avg(col(\"ee.age\")),2).alias(\"average_age\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5e2392-6c47-40ed-b4d1-0b9b863ce2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----------+\n|employee_id|   name|reports_count|average_age|\n+-----------+-------+-------------+-----------+\n|          1|Michael|            2|       40.0|\n|          2|  Alice|            2|       37.0|\n|          3|    Bob|            1|       37.0|\n+-----------+-------+-------------+-----------+\n\n+-----------+-------+-------------+-----------+\n|employee_id|   name|reports_count|average_age|\n+-----------+-------+-------------+-----------+\n|          1|Michael|            2|       40.0|\n|          2|  Alice|            2|       37.0|\n|          3|    Bob|            1|       37.0|\n+-----------+-------+-------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 30: SQL\n",
    "\n",
    "df30_2.createOrReplaceTempView(\"df30_2_tbl\")\n",
    "spark.sql('''\n",
    "          select a.employee_id, a.name, b.reports_count, b.average_age from df30_2_tbl as a inner join(\n",
    "          select reports_to, count(reports_to) as reports_count, round(avg(age),0) as average_age from df30_2_tbl group by reports_to) b\n",
    "          on a.employee_id=b.reports_to\n",
    "          ''').show()\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    "          select e.employee_id, e.name, count(*) as reports_count, round(avg(f.age),0) as average_age\n",
    "            from df30_2_tbl e inner join df30_2_tbl f on e.employee_id=f.reports_to\n",
    "            group by e.employee_id, e.name\n",
    "            order by e.employee_id;\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbdf0848-335c-4a12-9972-c427f6e1b934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 31: Primary Department for Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34be95ec-2cab-4afa-a7bf-c16cd25be647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+\n|employee_id|department_id|primary_flag|\n+-----------+-------------+------------+\n|          1|            1|           N|\n|          2|            1|           Y|\n|          2|            2|           N|\n|          3|            3|           N|\n|          4|            2|           N|\n|          4|            3|           Y|\n|          4|            4|           N|\n+-----------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data for DataFrame 31\n",
    "df31_data = [\n",
    "    (1, 1, 'N'),\n",
    "    (2, 1, 'Y'),\n",
    "    (2, 2, 'N'),\n",
    "    (3, 3, 'N'),\n",
    "    (4, 2, 'N'),\n",
    "    (4, 3, 'Y'),\n",
    "    (4, 4, 'N')\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "df31_schema = [\"employee_id\", \"department_id\", \"primary_flag\"]\n",
    "\n",
    "# Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is 'N'.\n",
    "# Write a solution to report all the employees with their primary department. For employees who belong to one department, report their only department.\n",
    "\n",
    "df31 = spark.createDataFrame(df31_data, df31_schema)\n",
    "df31.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32ad4a3-fe5e-4535-bf37-d8e023deaf4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|employee_id|department_id|\n+-----------+-------------+\n|          1|            1|\n|          3|            3|\n+-----------+-------------+\n\n+-----------+-------------+\n|employee_id|department_id|\n+-----------+-------------+\n|          2|            1|\n|          4|            3|\n|          1|            1|\n|          3|            3|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 31: SPARK\n",
    "\n",
    "df31_1 = df31.select(col(\"employee_id\"), col(\"department_id\")).filter(col(\"primary_flag\") == 'Y')\n",
    "\n",
    "df31_2 = df31.groupBy(col(\"employee_id\")).agg(count(col(\"department_id\")).alias(\"count_dept\")).filter(col(\"count_dept\") == 1)\\\n",
    "    .join(df31, on=\"employee_id\").select(col(\"employee_id\"), col(\"department_id\"))\n",
    "\n",
    "df31_2.show()\n",
    "\n",
    "df31_1.union(df31_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fede6897-3c89-44e0-8cb3-2b1b9d8aad85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|employee_id|department_id|\n+-----------+-------------+\n|          2|            1|\n|          4|            3|\n|          1|            1|\n|          3|            3|\n+-----------+-------------+\n\n+-----------+-------------+\n|employee_id|department_id|\n+-----------+-------------+\n|          2|            1|\n|          4|            3|\n|          1|            1|\n|          3|            3|\n+-----------+-------------+\n\n+-----------+-------------+\n|employee_id|department_id|\n+-----------+-------------+\n|          1|            1|\n|          2|            1|\n|          3|            3|\n|          4|            3|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 31: SQL\n",
    "\n",
    "df31.createOrReplaceTempView(\"df31_tbl\")\n",
    "\n",
    "# SLOW\n",
    "spark.sql('''\n",
    "          (select employee_id, department_id from df31_tbl where employee_id IN(\n",
    "          select employee_id from df31_tbl group by employee_id having count(department_id)>1) AND primary_flag='Y')\n",
    "          UNION\n",
    "          (select employee_id, department_id from df31_tbl where employee_id IN(\n",
    "          select employee_id from df31_tbl group by employee_id having count(department_id)=1))\n",
    "          ''').show()\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    "          select employee_id, department_id from df31_tbl where primary_flag='Y'\n",
    "          union\n",
    "          select employee_id, MAX(department_id) from df31_tbl group by employee_id having count(*) =1\n",
    "          ''').show()\n",
    "\n",
    "spark.sql('''\n",
    "         select employee_id, department_id from df31_tbl where primary_flag='Y' OR employee_id IN (\n",
    "             select employee_id from df31_tbl group by employee_id having count(employee_id) =1\n",
    "         )\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db4468d-26f4-4154-8452-453f3ec04dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 32: Triangle Judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e53ea9-10d6-4e2d-84ff-0ffe61d1eddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n|  x|  y|  z|\n+---+---+---+\n| 13| 15| 30|\n| 10| 20| 15|\n+---+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "df32_data = [\n",
    "    (13, 15, 30),\n",
    "    (10, 20, 15)\n",
    "]\n",
    "\n",
    "df32_schema = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "df32 = spark.createDataFrame(df32_data, df32_schema)\n",
    "\n",
    "# Report for every three line segments whether they can form a triangle.\n",
    "\n",
    "df32.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee0282c-05c4-47d9-bacc-8c3233dfb348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n|  x|  y|  z|triangle|\n+---+---+---+--------+\n| 13| 15| 30|      No|\n| 10| 20| 15|     Yes|\n+---+---+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 32: SPARK\n",
    "\n",
    "df32.select( \"*\", when(((col(\"x\")+col(\"y\") > col(\"z\")) & (col(\"x\")+col(\"z\") > col(\"y\")) & (col(\"y\")+col(\"z\") > col(\"x\"))), \"Yes\").otherwise(\"No\").alias(\"triangle\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7ee375-4c30-45fe-80cf-5f74a83fdba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+\n|  x|  y|  z|triangle|\n+---+---+---+--------+\n| 13| 15| 30|      No|\n| 10| 20| 15|     Yes|\n+---+---+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 32: SQL\n",
    "\n",
    "df32.createOrReplaceTempView(\"df32_tbl\")\n",
    "spark.sql('''\n",
    "          select * , (case when ((x+y>z) and (y+z>x) and (z+x>y)) then \"Yes\" else \"No\" end) as triangle from df32_tbl\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fffca399-2f67-425d-911f-078fad5ceca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 33: Consecutive Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc33ea8-2478-40d2-9888-481f507559e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n| id|num|\n+---+---+\n|  1|  1|\n|  2|  1|\n|  3|  1|\n|  4|  2|\n|  5|  1|\n|  6|  2|\n|  7|  2|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "df33_data = [\n",
    "    (1, \"1\"),\n",
    "    (2, \"1\"),\n",
    "    (3, \"1\"),\n",
    "    (4, \"2\"),\n",
    "    (5, \"1\"),\n",
    "    (6, \"2\"),\n",
    "    (7, \"2\")\n",
    "]\n",
    "\n",
    "df33_schema = [\"id\", \"num\"]\n",
    "\n",
    "# Find all numbers that appear at least three times consecutively.\n",
    "\n",
    "df33 = spark.createDataFrame(df33_data, df33_schema)\n",
    "\n",
    "df33.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bccfb6-0644-4cf8-9b59-8684015e031f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n| id|num|lead_1|lead_2|\n+---+---+------+------+\n|  1|  1|     1|     1|\n|  2|  1|     1|     2|\n|  3|  1|     2|     1|\n|  4|  2|     1|     2|\n|  5|  1|     2|     2|\n|  6|  2|     2|  null|\n|  7|  2|  null|  null|\n+---+---+------+------+\n\n+------------------+\n|consecutiveNumbers|\n+------------------+\n|                 1|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 33: SPARK\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win = Window.orderBy(col(\"id\"))\n",
    "\n",
    "df33.withColumn(\"lead_1\", lead(col(\"num\")).over(win)).withColumn(\"lead_2\", lead(col(\"num\"),2).over(win)).show()\n",
    "\n",
    "df33.withColumn(\"consecutiveNumbers\", when(((col(\"num\") == lead(col(\"num\")).over(win))  & (col(\"num\") == lead(col(\"num\"),2).over(win))), col(\"num\")))\\\n",
    "    .filter(col(\"consecutiveNumbers\").isNotNull())\\\n",
    "        .select(col(\"consecutiveNumbers\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82e4ae1-e175-453f-a60c-23a86d89faf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n| id|num|lead_1|lead_2|\n+---+---+------+------+\n|  1|  1|     1|     1|\n|  2|  1|     1|     2|\n|  3|  1|     2|     1|\n|  4|  2|     1|     2|\n|  5|  1|     2|     2|\n|  6|  2|     2|  null|\n|  7|  2|  null|  null|\n+---+---+------+------+\n\n+------------------+\n|consecutiveNumbers|\n+------------------+\n|                 1|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 33: SQL\n",
    "\n",
    "df33.createOrReplaceTempView(\"df33_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select id, num, lead(num) over(order by id) as lead_1, lead(num,2)  over(order by id) as lead_2 \n",
    "          from df33_tbl\n",
    "          ''').show()\n",
    "\n",
    "spark.sql('''\n",
    "          select consecutiveNumbers from(\n",
    "          select case when (num = lead(num) over(order by id) = lead(num,2)  over(order by id)) then num end as consecutiveNumbers\n",
    "          from df33_tbl)  where consecutiveNumbers IS NOT NULL\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1757d0ab-2966-4cae-80df-d7b5b46f2393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 34: Product Price at a Given Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0803a9fd-ca6f-4871-8f1f-c06d05187acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n|product_id|new_price|change_date|\n+----------+---------+-----------+\n|         1|       20| 2019-08-14|\n|         2|       50| 2019-08-14|\n|         1|       30| 2019-08-15|\n|         1|       35| 2019-08-16|\n|         2|       65| 2019-08-17|\n|         3|       20| 2019-08-18|\n+----------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df34_data = [\n",
    "    (1, 20, \"2019-08-14\"),\n",
    "    (2, 50, \"2019-08-14\"),\n",
    "    (1, 30, \"2019-08-15\"),\n",
    "    (1, 35, \"2019-08-16\"),\n",
    "    (2, 65, \"2019-08-17\"),\n",
    "    (3, 20, \"2019-08-18\")\n",
    "]\n",
    "\n",
    "df34_schema = [\"product_id\", \"new_price\", \"change_date\"]\n",
    "\n",
    "df34 = spark.createDataFrame(df34_data, df34_schema)\n",
    "\n",
    "# Initially, all products have price 10.\n",
    "# Write a solution to find the prices of all products on the date 2019-08-16.\n",
    "\n",
    "df34.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57436380-eb0e-405e-a95c-4c652ab4039e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|product_id|new_price|\n+----------+---------+\n|         2|       50|\n|         1|       35|\n+----------+---------+\n\n+----------+---------+\n|product_id|new_price|\n+----------+---------+\n|         3|       10|\n+----------+---------+\n\n+----------+---------+\n|product_id|new_price|\n+----------+---------+\n|         2|       50|\n|         1|       35|\n|         3|       10|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 34: SPARK\n",
    "\n",
    "df34_1 = df34.alias(\"a\").filter(col(\"a.change_date\")<= '2019-08-16').groupBy(col(\"a.product_id\")).agg(max(col(\"a.change_date\")).alias(\"max_date\"))\\\n",
    "    .join(df34.alias(\"b\"), (col(\"a.product_id\") == col(\"b.product_id\")) & (col(\"max_date\") == col(\"b.change_date\")))\\\n",
    "        .select(col(\"a.product_id\"), col(\"b.new_price\"))\n",
    "df34_1.show()\n",
    "\n",
    "df34_2 = df34.groupBy(col(\"product_id\")).agg(min(col(\"change_date\")).alias(\"min_date\")).filter(col(\"min_date\")> '2019-08-16').select(col(\"product_id\"), lit(10).alias(\"new_price\"))\n",
    "df34_2.show()\n",
    "\n",
    "df34_1.union(df34_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc21601-78ea-420c-9243-6e3fd260608f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|product_id|new_price|\n+----------+---------+\n|         2|       50|\n|         1|       35|\n+----------+---------+\n\n+----------+---------+\n|product_id|new_price|\n+----------+---------+\n|         3|       10|\n+----------+---------+\n\n+----------+---------+\n|product_id|new_price|\n+----------+---------+\n|         2|       50|\n|         1|       35|\n|         3|       10|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 34: SQL\n",
    "\n",
    "df34.createOrReplaceTempView(\"df34_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select product_id, new_price from df34_tbl where (product_id, change_date) IN (\n",
    "          select product_id, max(change_date) as max_date from df34_tbl where change_date <= '2019-08-16' group by product_id)\n",
    "          ''').show()\n",
    "\n",
    "spark.sql('''\n",
    "          select a.product_id, 10 as new_price from (\n",
    "          SELECT product_id, min(change_date) as min_date FROM df34_tbl group by product_id order by product_id) a inner join df34_tbl b\n",
    "          on a.product_id = b.product_id\n",
    "          where a.min_date > '2019-08-16'\n",
    "          ''').show()\n",
    "\n",
    "spark.sql('''\n",
    "          select product_id, new_price from df34_tbl where (product_id, change_date) IN (\n",
    "          select product_id, max(change_date) as max_date from df34_tbl where change_date <= '2019-08-16' group by product_id)\n",
    "          union\n",
    "          select a.product_id, 10 as new_price from (\n",
    "          SELECT product_id, min(change_date) as min_date FROM df34_tbl group by product_id order by product_id) a inner join df34_tbl b\n",
    "          on a.product_id = b.product_id\n",
    "          where a.min_date > '2019-08-16'\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a08a13f-e26c-44d0-909a-bf1f5dcc7a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 35: Last Person to Fit in the Bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12543c76-fde3-468a-bdb5-67db7b7d0959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+----+\n|person_id|person_name|weight|turn|\n+---------+-----------+------+----+\n|        5|      Alice|   250|   1|\n|        4|        Bob|   175|   5|\n|        3|       Alex|   350|   2|\n|        6|  John Cena|   400|   3|\n|        1|    Winston|   500|   6|\n|        2|      Marie|   200|   4|\n+---------+-----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "df35_data = [\n",
    "    (5, \"Alice\", 250, 1),\n",
    "    (4, \"Bob\", 175, 5),\n",
    "    (3, \"Alex\", 350, 2),\n",
    "    (6, \"John Cena\", 400, 3),\n",
    "    (1, \"Winston\", 500, 6),\n",
    "    (2, \"Marie\", 200, 4)\n",
    "]\n",
    "\n",
    "df35_schema = [\"person_id\", \"person_name\", \"weight\", \"turn\"]\n",
    "\n",
    "# There is a queue of people waiting to board a bus. However, the bus has a weight limit of 1000 kilograms, so there may be some people who cannot board.\n",
    "# Write a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.\n",
    "\n",
    "df35 = spark.createDataFrame(df35_data, df35_schema)\n",
    "df35.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18596b2b-e498-40b9-8927-3118b4a95341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|person_name|\n+-----------+\n|  John Cena|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 35: SPARK\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win = Window.orderBy(col(\"turn\"))\n",
    "\n",
    "df35.withColumn(\"consecutiveSum\", sum(col(\"weight\")).over(win))\\\n",
    "    .filter(col(\"consecutiveSum\") <= 1000 )\\\n",
    "        .collect()[-1][1]\n",
    "\n",
    "df35.withColumn(\"consecutiveSum\", sum(col(\"weight\")).over(win))\\\n",
    "    .filter(col(\"consecutiveSum\") <= 1000 )\\\n",
    "        .sort(col(\"consecutiveSum\").desc()).limit(1)\\\n",
    "            .select(col(\"person_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a4f372-4bbf-4889-b991-36c5e5cd4f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|person_name|\n+-----------+\n|  John Cena|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 35: SQL\n",
    "\n",
    "df35.createOrReplaceTempView(\"df35_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select person_name from (\n",
    "          select *, sum(weight) over(order by turn) as consecutiveSum from df35_tbl)\n",
    "          where consecutiveSum <= 1000\n",
    "          order by consecutiveSum desc\n",
    "          LIMIT 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a4c9a1-4352-40f7-8c57-9f613c546ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 36: Count Salary Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c15e1cf-a4b4-4e72-a342-b4c387bc45c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|account_id|income|\n+----------+------+\n|         3|108939|\n|         2| 12747|\n|         8| 87709|\n|         6| 91796|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df36_data = [\n",
    "    (3, 108939),\n",
    "    (2, 12747),\n",
    "    (8, 87709),\n",
    "    (6, 91796)\n",
    "]\n",
    "\n",
    "df36_schema = [\"account_id\", \"income\"]\n",
    "\n",
    "# Write a solution to calculate the number of bank accounts for each salary category. The salary categories are:\n",
    "# \"Low Salary\": All the salaries strictly less than $20000.\n",
    "# \"Average Salary\": All the salaries in the inclusive range [$20000, $50000].\n",
    "# \"High Salary\": All the salaries strictly greater than $50000.\n",
    "# The result table must contain all three categories. If there are no accounts in a category, return 0.\n",
    "\n",
    "df36 = spark.createDataFrame(df36_data, df36_schema)\n",
    "df36.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed42d04-6d6b-43a1-85b6-46f603c0ce46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n|      category|count(category)|\n+--------------+---------------+\n|    Low Salary|              1|\n|Average Salary|              0|\n|   High Salary|              3|\n+--------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 36: SPARK\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df36_count = df36.select(col(\"income\"), \n",
    "            when(col(\"income\") <20000, \"Low Salary\")\n",
    "            .when((col(\"income\") >= 20000) & (col(\"income\") <= 50000), \"Average Salary\")\n",
    "            .when(col(\"income\") >50000, \"High Salary\").alias(\"category\"))\\\n",
    "                .groupBy(col(\"category\")).agg(count(col(\"category\")))\n",
    "\n",
    "df36_category = spark.createDataFrame([\n",
    "    Row(category=\"Low Salary\"),\n",
    "    Row(category=\"Average Salary\"),\n",
    "    Row(category=\"High Salary\")\n",
    "])\n",
    "\n",
    "# df36_category.join(df36_count, df36_category[\"category\"] == df36_count[\"category\"], \"left\").show()\n",
    "# AttributeError: 'NoneType' object has no attribute 'join'\n",
    "\n",
    "df36_category.join(df36_count, on=\"category\", how=\"left\").fillna(0).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd50428-82e8-4d24-9612-5f29e7060d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**AttributeError: 'NoneType' object has no attribute 'join'**\n",
    "\n",
    "We get this error when one or more DF in the join return null, meaning one of those or both DataFrames was not created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc149d14-e8aa-44fd-a425-fbca20bec5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|      category|accounts_count|\n+--------------+--------------+\n|    Low Salary|             1|\n|Average Salary|             0|\n|   High Salary|             3|\n+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 36: SQL\n",
    "\n",
    "df36.createOrReplaceTempView(\"df36_tbl\")\n",
    "spark.sql('''\n",
    "          WITH category AS (\n",
    "          select income, (case when income <20000 then \"Low Salary\" when (income >=20000 and income<=50000) then \"Average Salary\" when income>50000 then \"High Salary\" end) as category \n",
    "          from df36_tbl),\n",
    "\n",
    "          category_list AS(\n",
    "          select \"Low Salary\" AS category  \n",
    "          UNION ALL\n",
    "          select \"Average Salary\"\n",
    "          UNION ALL\n",
    "          select \"High Salary\" \n",
    "          )\n",
    "\n",
    "          select a.category, ifnull(b.accounts_count,0) as accounts_count  from category_list a left join\n",
    "          (SELECT category, count(category) as accounts_count FROM category group by category) b\n",
    "          on a.category=b.category\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a5a309-97b7-41fa-874e-b9e30c24ea78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# What is a CTE?\n",
    "# A CTE (Common Table Expression) is a temporary result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. It helps organize complex queries, especially when you need to reuse intermediate results.\n",
    "\n",
    "# General Syntax\n",
    "\n",
    "WITH cte_name1 AS (\n",
    "    SELECT ...\n",
    "),\n",
    "cte_name2 AS (\n",
    "    SELECT ...\n",
    ")\n",
    "SELECT ...\n",
    "FROM cte_name1\n",
    "JOIN cte_name2 ON ...\n",
    "\n",
    "\n",
    "# You start with WITH.\n",
    "# Define one or more named blocks (CTEs). Each block is written like a mini-SELECT query inside parentheses.\n",
    "# After all the CTEs are defined, you write the main query that uses these CTEs.\n",
    "# select \"Low Salary\" AS category  . Here category is column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea83fe16-abca-4983-8450-aac0e6165404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 37: Employees Whose Manager Left the Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33079f8a-b645-4e19-b239-1324ac5bf80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+\n|employee_id|     name|manager_id|salary|\n+-----------+---------+----------+------+\n|          3|     Mila|         9| 60301|\n|         12|Antonella|      null| 31000|\n|         13|    Emery|      null| 67084|\n|          1|    Kalel|        11| 21241|\n|          9|  Mikaela|      null| 50937|\n|         11|   Joziah|         6| 28485|\n+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df37_data = [\n",
    "    (3, \"Mila\", 9, 60301),\n",
    "    (12, \"Antonella\", None, 31000),\n",
    "    (13, \"Emery\", None, 67084),\n",
    "    (1, \"Kalel\", 11, 21241),\n",
    "    (9, \"Mikaela\", None, 50937),\n",
    "    (11, \"Joziah\", 6, 28485)\n",
    "]\n",
    "\n",
    "df37_schema = [\"employee_id\", \"name\", \"manager_id\", \"salary\"]\n",
    "\n",
    "# This table contains information about the employees, their salary, and the ID of their manager. Some employees do not have a manager (manager_id is null). \n",
    "#  Find the IDs of the employees whose salary is strictly less than $30000 and whose manager left the company. When a manager leaves the company, their information is deleted from the Employees table, but the reports still have their manager_id set to the manager that left.\n",
    "# Return the result table ordered by employee_id.\n",
    "\n",
    "df37 = spark.createDataFrame(df37_data, df37_schema)\n",
    "df37.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584bc71a-dfa6-486b-bc18-6a89d753b47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          3|\n|         12|\n|         13|\n|          1|\n|          9|\n|         11|\n+-----------+\n\n+-----------+\n|employee_id|\n+-----------+\n|         11|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 37: SPARK\n",
    "df37_emp = df37.alias(\"a\")\n",
    "df37_distinct_emp = df37.select(col(\"employee_id\")).distinct().alias(\"b\")\n",
    "df37_distinct_emp.show()\n",
    "\n",
    "df37_emp.join(df37_distinct_emp, col(\"a.manager_id\") == col(\"b.employee_id\"), \"left_anti\" )\\\n",
    "    .filter((col(\"a.salary\")<30000) & (col(\"manager_id\").isNotNull()))\\\n",
    "        .select(col(\"a.employee_id\"))\\\n",
    "            .sort(col(\"a.employee_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47f10d6-4941-424b-b9e6-f292cff1964e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|         11|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 37: SQL\n",
    "\n",
    "df37.createOrReplaceTempView(\"df37_tbl\")\n",
    "spark.sql('''\n",
    "          select employee_id from df37_tbl where manager_id NOT IN (select employee_id from df37_tbl) and salary < 30000 order by employee_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6df4b83-1c89-4d9f-8092-d49f4e3cb90a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 38: Exchange Seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bec1fa5-74fe-4f20-be0a-982c2e49d3dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|student|\n+---+-------+\n|  1|  Abbot|\n|  2|  Doris|\n|  3|Emerson|\n|  4|  Green|\n|  5| Jeames|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df38_data = [\n",
    "    (1, \"Abbot\"),\n",
    "    (2, \"Doris\"),\n",
    "    (3, \"Emerson\"),\n",
    "    (4, \"Green\"),\n",
    "    (5, \"Jeames\")\n",
    "]\n",
    "\n",
    "df38_schema = [\"id\", \"student\"]\n",
    "\n",
    "# Write a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.\n",
    "# Return the result table ordered by id in ascending order.\n",
    "\n",
    "df38 = spark.createDataFrame(df38_data, df38_schema)\n",
    "df38.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365eebbd-6ce3-47a6-ad0b-2d3fefcc7707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n+------+-------+\n|new_id|student|\n+------+-------+\n|     1|  Doris|\n|     2|  Abbot|\n|     3|  Green|\n|     4|Emerson|\n|     5| Jeames|\n+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 38: SPARK\n",
    "\n",
    "df38_total_rows = df38.count()\n",
    "print(df38_total_rows)\n",
    "\n",
    "df38.withColumn(\"new_id\", when( ((col(\"id\")%2 == 1) & (col(\"id\") <df38_total_rows) ), col(\"id\")+1 )\n",
    "                .when(col(\"id\") %2 == 0, col(\"id\")-1)\n",
    "                .otherwise(col(\"id\")))\\\n",
    "                    .sort(col(\"new_id\"))\\\n",
    "                        .select(col(\"new_id\"), col(\"student\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8791f55b-a1d5-4f4d-9707-78cae866bf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|student|new_id|\n+-------+------+\n|  Doris|     1|\n|  Abbot|     2|\n|  Green|     3|\n|Emerson|     4|\n| Jeames|     5|\n+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 38: SQL\n",
    "df38.createOrReplaceTempView(\"df38_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select student , case when (id % 2 == 1) and ((id+1) <= (select count(*) from df38_tbl) ) then id+1\n",
    "          when (id % 2 == 0) then id-1\n",
    "          else id\n",
    "          end as new_id\n",
    "          from df38_tbl\n",
    "          order by new_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "874425b7-09f3-46ac-975d-24f8fc367d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 39: Movie Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3816ff1-c2de-4713-8fd8-16a31bd68462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|movie_id|   title|\n+--------+--------+\n|       1|Avengers|\n|       2|Frozen 2|\n|       3|   Joker|\n+--------+--------+\n\n+-------+------+\n|user_id|  name|\n+-------+------+\n|      1|Daniel|\n|      2|Monica|\n|      3| Maria|\n|      4| James|\n+-------+------+\n\n+--------+-------+------+----------+\n|movie_id|user_id|rating|created_at|\n+--------+-------+------+----------+\n|       1|      1|     3|2020-01-12|\n|       1|      2|     4|2020-02-11|\n|       1|      3|     2|2020-02-12|\n|       1|      4|     1|2020-01-01|\n|       2|      1|     5|2020-02-17|\n|       2|      2|     2|2020-02-01|\n|       2|      3|     2|2020-03-01|\n|       3|      1|     3|2020-02-22|\n|       3|      2|     4|2020-02-25|\n+--------+-------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df39_movies_data = [\n",
    "    (1, \"Avengers\"),\n",
    "    (2, \"Frozen 2\"),\n",
    "    (3, \"Joker\")\n",
    "]\n",
    "df39_movies_schema = [\"movie_id\", \"title\"]\n",
    "df39_movies = spark.createDataFrame(df39_movies_data, df39_movies_schema)\n",
    "\n",
    "df39_users_data = [\n",
    "    (1, \"Daniel\"),\n",
    "    (2, \"Monica\"),\n",
    "    (3, \"Maria\"),\n",
    "    (4, \"James\")\n",
    "]\n",
    "df39_users_schema = [\"user_id\", \"name\"]\n",
    "df39_users = spark.createDataFrame(df39_users_data, df39_users_schema)\n",
    "\n",
    "df39_ratings_data = [\n",
    "    (1, 1, 3, \"2020-01-12\"),\n",
    "    (1, 2, 4, \"2020-02-11\"),\n",
    "    (1, 3, 2, \"2020-02-12\"),\n",
    "    (1, 4, 1, \"2020-01-01\"),\n",
    "    (2, 1, 5, \"2020-02-17\"),\n",
    "    (2, 2, 2, \"2020-02-01\"),\n",
    "    (2, 3, 2, \"2020-03-01\"),\n",
    "    (3, 1, 3, \"2020-02-22\"),\n",
    "    (3, 2, 4, \"2020-02-25\")\n",
    "]\n",
    "df39_ratings_schema = [\"movie_id\", \"user_id\", \"rating\", \"created_at\"]\n",
    "df39_ratings = spark.createDataFrame(df39_ratings_data, df39_ratings_schema)\n",
    "\n",
    "# Write a solution to:\n",
    "# Find the name of the user who has rated the greatest number of movies. In case of a tie, return the lexicographically smaller user name.\n",
    "# Find the movie name with the highest average rating in February 2020. In case of a tie, return the lexicographically smaller movie name.\n",
    "\n",
    "df39_movies.show()\n",
    "df39_users.show()\n",
    "df39_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de015bb-516a-4c6f-aa38-efc8cae1c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|  result|\n+--------+\n|  Daniel|\n|Frozen 2|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 39: SPARK\n",
    "\n",
    "df39_res1 = df39_ratings.join(df39_users, on=\"user_id\", how=\"inner\")\\\n",
    "    .groupBy(col(\"user_id\"), col(\"name\")).agg(count(\"*\").alias(\"user_rated_movie\"))\\\n",
    "        .sort(col(\"user_rated_movie\").desc(), col(\"name\").asc())\\\n",
    "            .select(col(\"name\").alias(\"result\")).limit(1)\n",
    "\n",
    "\n",
    "df39_res2 = df39_ratings.join(df39_movies, on=\"movie_id\", how=\"inner\")\\\n",
    "    .withColumn(\"created_date\", to_date(\"created_at\"))\\\n",
    "        .filter(((year(col(\"created_date\")) == 2020)  &  (month(col(\"created_date\")) == 2)))\\\n",
    "            .groupBy(col(\"movie_id\"), col(\"title\")).agg(avg(col(\"rating\")).alias(\"avg_movie_rating\"))\\\n",
    "                .sort(col(\"avg_movie_rating\").desc(), col(\"title\").asc())\\\n",
    "                    .select(col(\"title\").alias(\"result\")).limit(1)\n",
    "\n",
    "df39_res1.union(df39_res2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7fd255-0daa-4c08-ab14-f5ba4b35675a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|result|\n+------+\n|Daniel|\n+------+\n\n+--------+\n|  result|\n+--------+\n|Frozen 2|\n+--------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1926884270911581>:27\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124m          select a.name as result from df38_u_tbl a inner join (\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124m          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     15\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124m          select c.title as result from df38_m_tbl c inner join (\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124m          select movie_id, avg(rating) as avg_rating from df38_r_tbl\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[0;32m---> 27\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;124m          select a.name as result from df38_u_tbl a inner join (\u001B[39m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124m          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\u001B[39m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124m          on a.user_id =b.user_id\u001B[39m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;124m          order by rating desc, name\u001B[39m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n",
       "\u001B[1;32m     33\u001B[0m \u001B[38;5;124m          \u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;124m          UNION\u001B[39m\n",
       "\u001B[1;32m     35\u001B[0m \n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;124m          select c.title as result from df38_m_tbl c inner join (\u001B[39m\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;124m          select movie_id, avg(rating) as avg_rating from df38_r_tbl\u001B[39m\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;124m          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\u001B[39m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;124m          group by movie_id) d\u001B[39m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;124m          on c.movie_id = d.movie_id\u001B[39m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;124m          order by d.avg_rating desc, c.title\u001B[39m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mParseException\u001B[0m: \n",
       "[PARSE_SYNTAX_ERROR] Syntax error at or near 'UNION'.(line 8, pos 10)\n",
       "\n",
       "== SQL ==\n",
       "\n",
       "          select a.name as result from df38_u_tbl a inner join (\n",
       "          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\n",
       "          on a.user_id =b.user_id\n",
       "          order by rating desc, name\n",
       "          limit 1\n",
       "          \n",
       "          UNION\n",
       "----------^^^\n",
       "\n",
       "          select c.title as result from df38_m_tbl c inner join (\n",
       "          select movie_id, avg(rating) as avg_rating from df38_r_tbl\n",
       "          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\n",
       "          group by movie_id) d\n",
       "          on c.movie_id = d.movie_id\n",
       "          order by d.avg_rating desc, c.title\n",
       "          limit 1\n",
       "          \n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1926884270911581>:27\u001B[0m\n\u001B[1;32m      7\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m          select a.name as result from df38_u_tbl a inner join (\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     15\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124m          select c.title as result from df38_m_tbl c inner join (\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m          select movie_id, avg(rating) as avg_rating from df38_r_tbl\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m---> 27\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124m          select a.name as result from df38_u_tbl a inner join (\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124m          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\u001B[39m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124m          on a.user_id =b.user_id\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124m          order by rating desc, name\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124m          \u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124m          UNION\u001B[39m\n\u001B[1;32m     35\u001B[0m \n\u001B[1;32m     36\u001B[0m \u001B[38;5;124m          select c.title as result from df38_m_tbl c inner join (\u001B[39m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124m          select movie_id, avg(rating) as avg_rating from df38_r_tbl\u001B[39m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124m          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\u001B[39m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124m          group by movie_id) d\u001B[39m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124m          on c.movie_id = d.movie_id\u001B[39m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124m          order by d.avg_rating desc, c.title\u001B[39m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124m          limit 1\u001B[39m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m'''\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'UNION'.(line 8, pos 10)\n\n== SQL ==\n\n          select a.name as result from df38_u_tbl a inner join (\n          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\n          on a.user_id =b.user_id\n          order by rating desc, name\n          limit 1\n          \n          UNION\n----------^^^\n\n          select c.title as result from df38_m_tbl c inner join (\n          select movie_id, avg(rating) as avg_rating from df38_r_tbl\n          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\n          group by movie_id) d\n          on c.movie_id = d.movie_id\n          order by d.avg_rating desc, c.title\n          limit 1\n          \n",
       "errorSummary": "<span class='ansi-red-fg'>ParseException</span>: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'UNION'.(line 8, pos 10)\n\n== SQL ==\n\n          select a.name as result from df38_u_tbl a inner join (\n          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\n          on a.user_id =b.user_id\n          order by rating desc, name\n          limit 1\n          \n          UNION\n----------^^^\n\n          select c.title as result from df38_m_tbl c inner join (\n          select movie_id, avg(rating) as avg_rating from df38_r_tbl\n          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\n          group by movie_id) d\n          on c.movie_id = d.movie_id\n          order by d.avg_rating desc, c.title\n          limit 1\n          \n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QUESTION 39: SQL\n",
    "\n",
    "df39_ratings.createOrReplaceTempView(\"df38_r_tbl\")\n",
    "df39_users.createOrReplaceTempView(\"df38_u_tbl\")\n",
    "df39_movies.createOrReplaceTempView(\"df38_m_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select a.name as result from df38_u_tbl a inner join (\n",
    "          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\n",
    "          on a.user_id =b.user_id\n",
    "          order by rating desc, name\n",
    "          limit 1\n",
    "          ''').show()\n",
    "\n",
    "spark.sql('''\n",
    "          select c.title as result from df38_m_tbl c inner join (\n",
    "          select movie_id, avg(rating) as avg_rating from df38_r_tbl\n",
    "          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\n",
    "          group by movie_id) d\n",
    "          on c.movie_id = d.movie_id\n",
    "          order by d.avg_rating desc, c.title\n",
    "          limit 1\n",
    "          ''').show()\n",
    "\n",
    "\n",
    "\n",
    "spark.sql('''\n",
    "          select a.name as result from df38_u_tbl a inner join (\n",
    "          select user_id, count(*) as rating from df38_r_tbl group by user_id) b\n",
    "          on a.user_id =b.user_id\n",
    "          order by rating desc, name\n",
    "          limit 1\n",
    "          \n",
    "          UNION\n",
    "\n",
    "          select c.title as result from df38_m_tbl c inner join (\n",
    "          select movie_id, avg(rating) as avg_rating from df38_r_tbl\n",
    "          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\n",
    "          group by movie_id) d\n",
    "          on c.movie_id = d.movie_id\n",
    "          order by d.avg_rating desc, c.title\n",
    "          limit 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac677095-97aa-4a4c-b7e6-c68eec35819f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|  result|\n+--------+\n|  Daniel|\n|Frozen 2|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Getting a ParseException because in Spark SQL, when using UNION or UNION ALL, you cannot use ORDER BY and LIMIT within each subquery\n",
    "\n",
    "spark.sql('''\n",
    "    WITH res1 AS (\n",
    "    select a.name as result from df38_u_tbl a inner join (\n",
    "    select user_id, count(*) as rating from df38_r_tbl group by user_id) b\n",
    "    on a.user_id =b.user_id\n",
    "    order by rating desc, name\n",
    "    limit 1\n",
    "),\n",
    "res2 AS (\n",
    "    select c.title as result from df38_m_tbl c inner join (\n",
    "          select movie_id, avg(rating) as avg_rating from df38_r_tbl\n",
    "          where month(to_date(created_at))=2 and year(to_date(created_at))=2020\n",
    "          group by movie_id) d\n",
    "          on c.movie_id = d.movie_id\n",
    "          order by d.avg_rating desc, c.title\n",
    "          limit 1\n",
    ")\n",
    "select * from res1 \n",
    "union\n",
    "select * from res2\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ce6978-1afc-4577-aecc-3e405e719a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 40: Restaurant Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af9b9bc-8c54-4043-8d36-23d84bd2e700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+------+\n|customer_id|   name|visited_on|amount|\n+-----------+-------+----------+------+\n|          1|   Jhon|2019-01-01|   100|\n|          2| Daniel|2019-01-02|   110|\n|          3|   Jade|2019-01-03|   120|\n|          4| Khaled|2019-01-04|   130|\n|          5|Winston|2019-01-05|   110|\n|          6|  Elvis|2019-01-06|   140|\n|          7|   Anna|2019-01-07|   150|\n|          8|  Maria|2019-01-08|    80|\n|          9|   Jaze|2019-01-09|   110|\n|          1|   Jhon|2019-01-10|   130|\n|          3|   Jade|2019-01-10|   150|\n+-----------+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df40_data = [\n",
    "    (1, \"Jhon\",    \"2019-01-01\", 100),\n",
    "    (2, \"Daniel\",  \"2019-01-02\", 110),\n",
    "    (3, \"Jade\",    \"2019-01-03\", 120),\n",
    "    (4, \"Khaled\",  \"2019-01-04\", 130),\n",
    "    (5, \"Winston\", \"2019-01-05\", 110),\n",
    "    (6, \"Elvis\",   \"2019-01-06\", 140),\n",
    "    (7, \"Anna\",    \"2019-01-07\", 150),\n",
    "    (8, \"Maria\",   \"2019-01-08\", 80),\n",
    "    (9, \"Jaze\",    \"2019-01-09\", 110),\n",
    "    (1, \"Jhon\",    \"2019-01-10\", 130),\n",
    "    (3, \"Jade\",    \"2019-01-10\", 150),\n",
    "]\n",
    "df40_schema = [\"customer_id\", \"name\", \"visited_on\", \"amount\"]\n",
    "df40 = spark.createDataFrame(df40_data, df40_schema).withColumn(\"visited_on\", to_date(\"visited_on\"))\n",
    "\n",
    "# Compute the moving average of how much the customer paid in a seven days window (i.e., current day + 6 days before). average_amount should be rounded to two decimal places.\n",
    "# Return the result table ordered by visited_on in ascending order.\n",
    "\n",
    "df40.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0aa5d46-4df3-4323-b336-7838aa908872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n|visited_date|moving_sum|moving_avg|\n+------------+----------+----------+\n|  2019-01-07|       860|    122.86|\n|  2019-01-08|       840|     120.0|\n|  2019-01-09|       840|     120.0|\n|  2019-01-10|      1000|    142.86|\n+------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 40: SPARK\n",
    "\n",
    "# NOTE: On a single day, we can have multiple customers. So first, find total amount against each date by using groupby\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win1 = Window.orderBy(\"visited_date\").rowsBetween(-6,0)\n",
    "win2 = Window.orderBy(\"visited_date\")\n",
    "\n",
    "df40.withColumn(\"visited_date\", to_date(\"visited_on\"))\\\n",
    "    .groupBy(col(\"visited_date\")).agg(sum(col(\"amount\")).alias(\"total_amt\"))\\\n",
    "        .withColumn(\"moving_avg\", round(avg(col(\"total_amt\")).over(win1),2))\\\n",
    "            .withColumn(\"moving_sum\", round(sum(col(\"total_amt\")).over(win1),2))\\\n",
    "                .withColumn(\"row_number\", row_number().over(win2))\\\n",
    "                    .filter(col(\"row_number\")>6)\\\n",
    "                        .select(\"visited_date\", \"moving_sum\", \"moving_avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79983594-945d-43bd-95a4-9d794b2e30d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n|visited_date|moving_sum|moving_avg|\n+------------+----------+----------+\n|  2019-01-07|       860|    122.86|\n|  2019-01-08|       840|     120.0|\n|  2019-01-09|       840|     120.0|\n|  2019-01-10|      1000|    142.86|\n+------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 40: SQL\n",
    "\n",
    "df40.createOrReplaceTempView(\"df40_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select a.visited_date, sum(a.total_sum) over(order by a.visited_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_sum,\n",
    "          round(avg(a.total_sum) over(order by a.visited_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),2) as moving_avg\n",
    "          from (\n",
    "          select to_date(visited_on) as visited_date, sum(amount) as total_sum from df40_tbl group by visited_on ) AS a\n",
    "          OFFSET 6\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d87cc9-b953-4b49-9eea-bae97e026ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 41: Friend Requests II: Who Has the Most Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ccfeb03-91fc-4608-8b20-edee062f5b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+\n|requester_id|accepter_id|accept_date|\n+------------+-----------+-----------+\n|           1|          2| 2016/06/03|\n|           1|          3| 2016/06/08|\n|           2|          3| 2016/06/08|\n|           3|          4| 2016/06/09|\n+------------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df41_data = [\n",
    "    (1, 2, \"2016/06/03\"),\n",
    "    (1, 3, \"2016/06/08\"),\n",
    "    (2, 3, \"2016/06/08\"),\n",
    "    (3, 4, \"2016/06/09\"),\n",
    "]\n",
    "df41_schema = [\"requester_id\", \"accepter_id\", \"accept_date\"]\n",
    "\n",
    "df41 = spark.createDataFrame(df41_data, df41_schema)\n",
    "\n",
    "# Write a solution to find the people who have the most friends and the most friends number.\n",
    "# The test cases are generated so that only one person has the most friends.\n",
    "\n",
    "df41.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc644411-df7c-4a92-a064-0fa76ee117f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n|ids|count_ids|\n+---+---------+\n|  3|        3|\n+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 41: SPARK\n",
    "\n",
    "df41_req = df41.select(col(\"requester_id\").alias(\"ids\"))\n",
    "df41_acc = df41.select(col(\"accepter_id\").alias(\"ids\"))\n",
    "\n",
    "df_req_total = df41_req.union(df41_acc)\\\n",
    "    .groupBy(col(\"ids\")).agg(count(\"*\").alias(\"count_ids\"))\\\n",
    "        .sort(col(\"count_ids\").desc())\n",
    "\n",
    "max_output = df_req_total.collect()[0][1]\n",
    "\n",
    "df_req_total.filter(col(\"count_ids\") == max_output).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b74466-38e6-4ec7-a20d-ab29d6a5d477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n|ids|total_ids|\n+---+---------+\n|  3|        3|\n+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 41: SQL\n",
    "\n",
    "df41.createOrReplaceTempView(\"df41_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          WITH df41_acc_req AS (\n",
    "              select requester_id from df41_tbl\n",
    "              UNION ALL\n",
    "              select accepter_id from df41_tbl\n",
    "          )\n",
    "          SELECT requester_id as ids, count(*) as total_ids FROM df41_acc_req group by requester_id\n",
    "          order by total_ids DESC LIMIT 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c8651c-a040-4bff-815c-450da6faa0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 42: Investments in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724b4ffc-199e-437d-9a26-88123a3020a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+----+----+\n|pid|tiv_2015|tiv_2016| lat| lon|\n+---+--------+--------+----+----+\n|  1|    10.0|     5.0|10.0|10.0|\n|  2|    20.0|    20.0|20.0|20.0|\n|  3|    10.0|    30.0|20.0|20.0|\n|  4|    10.0|    40.0|40.0|40.0|\n|  4|    30.0|    40.0|50.0|50.0|\n|  4|    30.0|    40.0|60.0|60.0|\n+---+--------+--------+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df42_data = [\n",
    "    (1, 10.0, 5.0, 10.0, 10.0),\n",
    "    (2, 20.0, 20.0, 20.0, 20.0),\n",
    "    (3, 10.0, 30.0, 20.0, 20.0),\n",
    "    (4, 10.0, 40.0, 40.0, 40.0),\n",
    "    (4, 30.0, 40.0, 50.0, 50.0),\n",
    "    (4, 30.0, 40.0, 60.0, 60.0),\n",
    "]\n",
    "df42_schema = [\"pid\", \"tiv_2015\", \"tiv_2016\", \"lat\", \"lon\"]\n",
    "\n",
    "df42 = spark.createDataFrame(df42_data, df42_schema)\n",
    "\n",
    "# Write a solution to report the sum of all total investment values in 2016 tiv_2016, for all policyholders who:\n",
    "# have the same tiv_2015 value as one or more other policyholders, and\n",
    "# are not located in the same city as any other policyholder (i.e., the (lat, lon) attribute pairs must be unique).\n",
    "# Round tiv_2016 to two decimal places.\n",
    "df42.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c3b581-b364-4e35-a6bf-a72b3b1d5414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|tiv__2016|\n+---------+\n|    125.0|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 42: SPARK\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win1 = Window.partitionBy(\"tiv_2015\")\n",
    "win2 = Window.partitionBy(\"lat\",\"lon\")\n",
    "\n",
    "df42.withColumn(\"tiv_count\", count(col(\"tiv_2015\")).over(win1))\\\n",
    "    .withColumn(\"lat_lon\", count(\"*\").over(win2))\\\n",
    "        .filter( ((col(\"tiv_count\") > 1) & (col(\"lat_lon\") ==1)) )\\\n",
    "            .select(sum(col(\"tiv_2016\")).alias(\"tiv__2016\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb43bdc-98b9-4afa-8246-13127531beb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|tiv_2016|\n+--------+\n|   125.0|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 42: SQL\n",
    "\n",
    "df42.createOrReplaceTempView(\"df24_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          WITH unique_lat_lon AS (\n",
    "              select * , count(*) over (partition by tiv_2015) as tiv_count,\n",
    "              count(*) over (partition by lat, lon) as lat_lon\n",
    "              from df24_tbl\n",
    "          )\n",
    "            SELECT sum(tiv_2016) as tiv_2016 FROM unique_lat_lon where tiv_count>1 and lat_lon =1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e91757f-3600-475f-82de-a342dbc2913f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 43: Department Top Three Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce6a8d5-d34c-4ba1-b22a-d6b92331652a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n| id| name|salary|departmentId|\n+---+-----+------+------------+\n|  1|  Joe| 85000|           1|\n|  2|Henry| 80000|           2|\n|  3|  Sam| 60000|           2|\n|  4|  Max| 90000|           1|\n|  5|Janet| 69000|           1|\n|  6|Randy| 85000|           1|\n|  7| Will| 70000|           1|\n+---+-----+------+------------+\n\n+---+-----+\n| id| name|\n+---+-----+\n|  1|   IT|\n|  2|Sales|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df43_employee_data = [\n",
    "    (1, \"Joe\",   85000, 1),\n",
    "    (2, \"Henry\", 80000, 2),\n",
    "    (3, \"Sam\",   60000, 2),\n",
    "    (4, \"Max\",   90000, 1),\n",
    "    (5, \"Janet\", 69000, 1),\n",
    "    (6, \"Randy\", 85000, 1),\n",
    "    (7, \"Will\",  70000, 1),\n",
    "]\n",
    "df43_employee_schema = [\"id\", \"name\", \"salary\", \"departmentId\"]\n",
    "df43_employee = spark.createDataFrame(df43_employee_data, df43_employee_schema)\n",
    "\n",
    "df43_department_data = [\n",
    "    (1, \"IT\"),\n",
    "    (2, \"Sales\"),\n",
    "]\n",
    "df43_department_schema = [\"id\", \"name\"]\n",
    "df43_department = spark.createDataFrame(df43_department_data, df43_department_schema)\n",
    "\n",
    "df43_employee.show()\n",
    "df43_department.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8678a2-c74f-4541-ac5d-6d7f88206e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n|Department|Employee|salary|\n+----------+--------+------+\n|        IT|     Max| 90000|\n|        IT|   Randy| 85000|\n|        IT|     Joe| 85000|\n|        IT|    Will| 70000|\n|     Sales|   Henry| 80000|\n|     Sales|     Sam| 60000|\n+----------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 43: SPARK\n",
    "\n",
    "df43_employee = df43_employee.alias(\"a\")\n",
    "df43_department = df43_department.alias(\"b\")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "win_dept = Window.partitionBy(col(\"departmentId\")).orderBy(col(\"salary\").desc())\n",
    "\n",
    "df43_employee.withColumn(\"rank_sal_dept\", dense_rank().over(win_dept))\\\n",
    "    .join(df43_department, col(\"a.departmentId\") == col(\"b.id\"))\\\n",
    "        .filter(col(\"rank_sal_dept\")<4)\\\n",
    "            .select(col(\"b.name\").alias(\"Department\"), col(\"a.name\").alias(\"Employee\"), col(\"a.salary\") )\\\n",
    "                .sort(col(\"departmentId\"), col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016e55f0-408c-4d80-95ea-0003a51693d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n|Department|Employee|salary|\n+----------+--------+------+\n|        IT|     Max| 90000|\n|        IT|     Joe| 85000|\n|        IT|   Randy| 85000|\n|        IT|    Will| 70000|\n|     Sales|   Henry| 80000|\n|     Sales|     Sam| 60000|\n+----------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 43: SQL\n",
    "\n",
    "df43_employee.createOrReplaceTempView(\"df43_e_tbl\")\n",
    "df43_department.createOrReplaceTempView(\"df43_d_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select a.name as Department, b.name as Employee, b.salary from df43_d_tbl a inner join (\n",
    "          select *, dense_rank() over(partition by departmentId order by salary desc ) as rank_sal from df43_e_tbl) b on a.id = b.departmentId\n",
    "          where rank_sal < 4\n",
    "          order by Department, b.salary desc\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37c4ac22-aa3b-4625-918c-80c60bfa398b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 44: Fix Names in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a665ea52-09c3-4863-8b52-c17f377e1b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|user_id| name|\n+-------+-----+\n|      1|aLice|\n|      2|  bOB|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df44_data = [\n",
    "    (1, \"aLice\"),\n",
    "    (2, \"bOB\"),\n",
    "]\n",
    "df44_schema = [\"user_id\", \"name\"]\n",
    "df44 = spark.createDataFrame(df44_data, df44_schema)\n",
    "\n",
    "# Write a solution to fix the names so that only the first character is uppercase and the rest are lowercase.\n",
    "# Return the result table ordered by user_id.\n",
    "\n",
    "df44.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dae90d-697b-467a-a78a-1c32bd073d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|user_id|name_new|\n+-------+--------+\n|      1|   Alice|\n|      2|     Bob|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 44: SPARK\n",
    "\n",
    "df44.select(col(\"user_id\"), concat(  upper(substring(col(\"name\"),1,1)) ,  lower(substring(col(\"name\"),2,100))  ).alias(\"name_new\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84314764-8aa3-42d7-b76a-dbcc95637132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|user_id|name_new|\n+-------+--------+\n|      1|   Alice|\n|      2|     Bob|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 44: SQL\n",
    "\n",
    "df44.createOrReplaceTempView(\"df44_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select user_id, concat(upper(left(name,1)), lower(substring(name,2))) as name_new from df44_tbl\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3615183b-91ba-44db-b7f5-867a65a1e9ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 45: Patients With a Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63730323-eac3-4607-8bed-0776573ccd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n|patient_id|patient_name|  conditions|\n+----------+------------+------------+\n|         1|      Daniel|  YFEV COUGH|\n|         2|       Alice|            |\n|         3|         Bob|DIAB100 MYOP|\n|         4|      George|ACNE DIAB100|\n|         5|       Alain|     DIAB201|\n+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df45_data = [\n",
    "    (1, \"Daniel\", \"YFEV COUGH\"),\n",
    "    (2, \"Alice\", \"\"),\n",
    "    (3, \"Bob\", \"DIAB100 MYOP\"),\n",
    "    (4, \"George\", \"ACNE DIAB100\"),\n",
    "    (5, \"Alain\", \"DIAB201\"),\n",
    "]\n",
    "df45_schema = [\"patient_id\", \"patient_name\", \"conditions\"]\n",
    "df45 = spark.createDataFrame(df45_data, df45_schema)\n",
    "\n",
    "# Write a solution to find the patient_id, patient_name, and conditions of the patients who have Type I Diabetes. Type I Diabetes always starts with DIAB1 prefix.\n",
    "\n",
    "df45.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1714a8a6-e352-4a1c-b0fb-ae6e72884bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n|patient_id|patient_name|  conditions|\n+----------+------------+------------+\n|         3|         Bob|DIAB100 MYOP|\n|         4|      George|ACNE DIAB100|\n+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 45: SPARK\n",
    "\n",
    "df45.filter(col(\"conditions\").like(\"%DIAB1%\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2aff32-fccd-4377-8e5c-a0608f64673f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n|patient_id|patient_name|  conditions|\n+----------+------------+------------+\n|         3|         Bob|DIAB100 MYOP|\n|         4|      George|ACNE DIAB100|\n+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 45: SQL\n",
    "\n",
    "df45.createOrReplaceTempView(\"df45_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select * from df45_tbl where conditions LIKE \"%DIAB1%\"\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d428bb5-4eca-4527-ad9f-7d2707345e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 46: Delete Duplicate Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c03ce3-9692-4167-b7fb-43cd1171d2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n| id|           email|\n+---+----------------+\n|  1|john@example.com|\n|  2| bob@example.com|\n|  3|john@example.com|\n+---+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df46_data = [\n",
    "    (1, \"john@example.com\"),\n",
    "    (2, \"bob@example.com\"),\n",
    "    (3, \"john@example.com\"),\n",
    "]\n",
    "df46_schema = [\"id\", \"email\"]\n",
    "df46 = spark.createDataFrame(df46_data, df46_schema)\n",
    "# Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id.\n",
    "\n",
    "df46.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c8d2bab-4f70-4b9f-aac0-95d0b9e96862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|           email|\n+----------------+\n|john@example.com|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 46: SPARK\n",
    "\n",
    "df46_1 = df46.alias(\"a\")\n",
    "df46_2 = df46.alias(\"b\")\n",
    "\n",
    "df46_1.join(df46_2, on=\"email\", how=\"inner\")\\\n",
    "    .filter(col(\"a.id\")<col(\"b.id\"))\\\n",
    "        .select(col(\"a.email\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e131442-a91d-4308-93ae-476343c551ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---+----------------+\n| id|           email| id|           email|\n+---+----------------+---+----------------+\n|  1|john@example.com|  3|john@example.com|\n+---+----------------+---+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 46: SQL\n",
    "\n",
    "df46.createOrReplaceTempView(\"df46_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select * from df46_tbl a inner join df46_tbl b on a.email = b.email and a.id<b.id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bdeb9dc-b203-4ae9-b0fa-08438dbcb6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 47: Second Highest Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4d41b5-1f51-4f65-bf70-db852eba6c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|salary|\n+---+------+\n|  1|   100|\n|  2|   200|\n|  3|   300|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df47_1_data = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 300),\n",
    "]\n",
    "df47_1_schema = [\"id\", \"salary\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df47_1 = spark.createDataFrame(df47_1_data, df47_1_schema)\n",
    "\n",
    "# Write a solution to find the second highest distinct salary from the Employee table. If there is no second highest salary, return null \n",
    "\n",
    "df47_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fa5dda-d8c3-4765-ab0f-669ef53802c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|salary|\n+------+\n|   200|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 47: SPARK\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win_salary = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df47_1.withColumn(\"rank_sal\", rank().over(win_salary))\\\n",
    "    .filter(col(\"rank_sal\") == 2)\\\n",
    "        .select(col(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6292e3da-36da-4976-9c0c-479ef7fca4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|max(salary)|\n+-----------+\n|        200|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 47: SQL\n",
    "\n",
    "df47_1.createOrReplaceTempView(\"df47_1_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select max(salary) from (\n",
    "          select *, rank() over(order by salary desc) as rank_sal from df47_1_tbl ) a\n",
    "          where rank_sal = 2\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7c13e6-4f85-47a3-a6c9-c2b899f937fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|salary|\n+---+------+\n|  1|   100|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df47_2_data = [\n",
    "    (1, 100),\n",
    "]\n",
    "df47_2_schema = [\"id\", \"salary\"]\n",
    "\n",
    "df47_2 = spark.createDataFrame(df47_2_data, df47_2_schema)\n",
    "df47_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dcddf96-d56f-4fcb-9c43-58e9a89911ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|salary|\n+------+\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 47: SPARK\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win_salary = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df47_2.withColumn(\"rank_sal\", rank().over(win_salary))\\\n",
    "    .filter(col(\"rank_sal\") == 2)\\\n",
    "        .select(col(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e9a973-445c-4f3e-99bf-2b57208d8e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|max(salary)|\n+-----------+\n|       null|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 47: SQL\n",
    "\n",
    "df47_2.createOrReplaceTempView(\"df47_2_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select max(salary) from (\n",
    "          select *, rank() over(order by salary desc) as rank_sal from df47_2_tbl ) a\n",
    "          where rank_sal = 2\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c25f48-90b2-4aa0-b0ef-2e522d9dcb83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 48: Group Sold Products By The Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2aaa604-4877-46aa-b65c-a82a07bc70ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n| sell_date|   product|\n+----------+----------+\n|2020-05-30| Headphone|\n|2020-06-01|    Pencil|\n|2020-06-02|      Mask|\n|2020-05-30|Basketball|\n|2020-06-01|     Bible|\n|2020-06-02|      Mask|\n|2020-05-30|   T-Shirt|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df48_data = [\n",
    "    (\"2020-05-30\", \"Headphone\"),\n",
    "    (\"2020-06-01\", \"Pencil\"),\n",
    "    (\"2020-06-02\", \"Mask\"),\n",
    "    (\"2020-05-30\", \"Basketball\"),\n",
    "    (\"2020-06-01\", \"Bible\"),\n",
    "    (\"2020-06-02\", \"Mask\"),\n",
    "    (\"2020-05-30\", \"T-Shirt\")\n",
    "]\n",
    "\n",
    "df48_schema = [\"sell_date\", \"product\"]\n",
    "\n",
    "df48 = spark.createDataFrame(df48_data, df48_schema)\n",
    "\n",
    "# Write a solution to find for each date the number of different products sold and their names.\n",
    "# The sold products names for each date should be sorted lexicographically.\n",
    "# Return the result table ordered by sell_date.\n",
    "\n",
    "df48.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374e32d4-1c35-47e4-aaf8-15e78efaf961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------+--------+\n|sell_date |products                        |num_sold|\n+----------+--------------------------------+--------+\n|2020-05-30|[Basketball, Headphone, T-Shirt]|3       |\n|2020-06-01|[Bible, Pencil]                 |2       |\n|2020-06-02|[Mask]                          |1       |\n+----------+--------------------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 48: SPARK\n",
    "\n",
    "df48.groupBy(\"sell_date\")\\\n",
    "    .agg(\n",
    "        sort_array(collect_set(\"product\")).alias(\"products\"),\n",
    "        size(collect_set(\"product\")).alias(\"num_sold\")\n",
    "    )\\\n",
    "    .orderBy(\"sell_date\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3f9859-9e5f-4d6f-8896-67b861998536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------------------------+\n|sell_date |num_sold|products                    |\n+----------+--------+----------------------------+\n|2020-05-30|3       |Basketball,Headphone,T-Shirt|\n|2020-06-01|2       |Bible,Pencil                |\n|2020-06-02|1       |Mask                        |\n+----------+--------+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 48: SQL\n",
    "\n",
    "df48.createOrReplaceTempView(\"df48_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT \n",
    "      sell_date, \n",
    "      SIZE(COLLECT_SET(product)) AS num_sold,\n",
    "      ARRAY_JOIN(SORT_ARRAY(COLLECT_SET(product)), ',') AS products\n",
    "    FROM df48_tbl\n",
    "    GROUP BY sell_date\n",
    "    ORDER BY sell_date\n",
    "''').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dc37262-3dc7-424b-9f1d-85e71548671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 49: List the Products Ordered in a Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5a1410-84a5-494e-b60f-c4142640334f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------+\n|product_id|        product_name|product_category|\n+----------+--------------------+----------------+\n|         1|  Leetcode Solutions|            Book|\n|         2|Jewels of Stringo...|            Book|\n|         3|                  HP|          Laptop|\n|         4|              Lenovo|          Laptop|\n|         5|        Leetcode Kit|         T-shirt|\n+----------+--------------------+----------------+\n\n+----------+----------+----+\n|product_id|order_date|unit|\n+----------+----------+----+\n|         1|2020-02-05|  60|\n|         1|2020-02-10|  70|\n|         2|2020-01-18|  30|\n|         2|2020-02-11|  80|\n|         3|2020-02-17|   2|\n|         3|2020-02-24|   3|\n|         4|2020-03-01|  20|\n|         4|2020-03-04|  30|\n|         4|2020-03-04|  60|\n|         5|2020-02-25|  50|\n|         5|2020-02-27|  50|\n|         5|2020-03-01|  50|\n+----------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Products DataFrame\n",
    "df49_products_data = [\n",
    "    (1, \"Leetcode Solutions\", \"Book\"),\n",
    "    (2, \"Jewels of Stringology\", \"Book\"),\n",
    "    (3, \"HP\", \"Laptop\"),\n",
    "    (4, \"Lenovo\", \"Laptop\"),\n",
    "    (5, \"Leetcode Kit\", \"T-shirt\")\n",
    "]\n",
    "df49_products_schema = [\"product_id\", \"product_name\", \"product_category\"]\n",
    "df49_products = spark.createDataFrame(df49_products_data, df49_products_schema)\n",
    "\n",
    "# Orders DataFrame\n",
    "df49_orders_data = [\n",
    "    (1, \"2020-02-05\", 60),\n",
    "    (1, \"2020-02-10\", 70),\n",
    "    (2, \"2020-01-18\", 30),\n",
    "    (2, \"2020-02-11\", 80),\n",
    "    (3, \"2020-02-17\", 2),\n",
    "    (3, \"2020-02-24\", 3),\n",
    "    (4, \"2020-03-01\", 20),\n",
    "    (4, \"2020-03-04\", 30),\n",
    "    (4, \"2020-03-04\", 60),\n",
    "    (5, \"2020-02-25\", 50),\n",
    "    (5, \"2020-02-27\", 50),\n",
    "    (5, \"2020-03-01\", 50)\n",
    "]\n",
    "df49_orders_schema = [\"product_id\", \"order_date\", \"unit\"]\n",
    "df49_orders = spark.createDataFrame(df49_orders_data, df49_orders_schema)\n",
    "\n",
    "# Write a solution to get the names of products that have at least 100 units ordered in February 2020 and their amount.\n",
    "\n",
    "df49_products.show()\n",
    "df49_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a69212-fd31-4bad-b863-4fdf851c89d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n|      product_name|units_sold|\n+------------------+----------+\n|Leetcode Solutions|       130|\n|      Leetcode Kit|       100|\n+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 49: SPARK\n",
    "\n",
    "df49_orders.withColumn(\"order_date_new\", to_date(col(\"order_date\")))\\\n",
    "    .filter(((month(col(\"order_date_new\")) == 2) & (year(col(\"order_date_new\"))==2020)))\\\n",
    "        .groupBy(col(\"product_id\")).agg(sum(col(\"unit\")).alias(\"units_sold\"))\\\n",
    "            .filter(col(\"units_sold\") >= 100)\\\n",
    "                .join(df49_products, on=\"product_id\", how=\"inner\")\\\n",
    "                    .select(col(\"product_name\"), col(\"units_sold\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a7a79d-8ef7-4a6e-b392-95e5be575e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n|      product_name|units|\n+------------------+-----+\n|Leetcode Solutions|  130|\n|      Leetcode Kit|  100|\n+------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 49: SQL\n",
    "\n",
    "df49_products.createOrReplaceTempView(\"df49_p_tbl\")\n",
    "df49_orders.createOrReplaceTempView(\"df49_o_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "          select product_name, units from df49_p_tbl a inner join (\n",
    "          select product_id, sum(unit) as units from df49_o_tbl\n",
    "          where month(to_date(order_date)) = 2 and year(to_date(order_date))=2020\n",
    "          group by product_id\n",
    "          having units>=100) b\n",
    "          on a.product_id=b.product_id\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3b92aff-f0c0-4a1b-8d17-1ad38817b2b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QUESTION 50: Find Users With Valid E-Mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d26e7df-e87b-4d90-8b1d-ef23d468e8a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------------------+\n|user_id|name     |mail                   |\n+-------+---------+-----------------------+\n|1      |Winston  |winston@leetcode.com   |\n|2      |Jonathan |jonathanisgreat        |\n|3      |Annabelle|bella-@leetcode.com    |\n|4      |Sally    |sally.come@leetcode.com|\n|5      |Marwan   |quarz#2020@leetcode.com|\n|6      |David    |david69@gmail.com      |\n|7      |Shapiro  |.shapo@leetcode.com    |\n+-------+---------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df50_data = [\n",
    "    (1, \"Winston\",   \"winston@leetcode.com\"),\n",
    "    (2, \"Jonathan\",  \"jonathanisgreat\"),\n",
    "    (3, \"Annabelle\", \"bella-@leetcode.com\"),\n",
    "    (4, \"Sally\",     \"sally.come@leetcode.com\"),\n",
    "    (5, \"Marwan\",    \"quarz#2020@leetcode.com\"),\n",
    "    (6, \"David\",     \"david69@gmail.com\"),\n",
    "    (7, \"Shapiro\",   \".shapo@leetcode.com\")\n",
    "]\n",
    "df50_schema = [\"user_id\", \"name\", \"mail\"]\n",
    "df50 = spark.createDataFrame(df50_data, df50_schema)\n",
    "\n",
    "# Write a solution to find the users who have valid emails.\n",
    "# A valid e-mail has a prefix name and a domain where:\n",
    "# The prefix name is a string that may contain letters (upper or lower case), digits, underscore '_', period '.', and/or dash '-'. The prefix name must start with a letter.\n",
    "# The domain is '@leetcode.com'.\n",
    "\n",
    "df50.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f050bfbd-2ee8-4072-a32f-96f9db0f6b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------------------+\n|user_id|name     |mail                   |\n+-------+---------+-----------------------+\n|1      |Winston  |winston@leetcode.com   |\n|3      |Annabelle|bella-@leetcode.com    |\n|4      |Sally    |sally.come@leetcode.com|\n+-------+---------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 50: SPARK\n",
    "\n",
    "# Regex pattern explanation:\n",
    "# ^           : start of string\n",
    "# [a-zA-Z]    : first character must be a letter\n",
    "# [\\w\\.-]*    : remaining characters can be word characters, dot or dash\n",
    "# @leetcode\\.com$ : ends with @leetcode.com\n",
    "\n",
    "email_pattern = r\"^[a-zA-Z][\\w\\.-]*@leetcode\\.com$\"\n",
    "\n",
    "df50_valid = df50.filter(col(\"mail\").rlike(email_pattern))\n",
    "df50_valid.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acaf4f3-1f60-49f5-92ff-b59ce89ba78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------------------+\n|user_id|name     |mail                   |\n+-------+---------+-----------------------+\n|1      |Winston  |winston@leetcode.com   |\n|3      |Annabelle|bella-@leetcode.com    |\n|4      |Sally    |sally.come@leetcode.com|\n+-------+---------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 50: SQL\n",
    "\n",
    "df50.createOrReplaceTempView(\"df50_tbl\")\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT * FROM df50_tbl\n",
    "WHERE mail RLIKE '^[a-zA-Z][a-zA-Z0-9._-]*@leetcode\\\\.com$'\n",
    "''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ea50fc-f0b7-42aa-854a-84801c521207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Keep your code lit — just like Spark! "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LeetCode_SQL_Spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}